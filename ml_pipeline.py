import numpy as np
import torch
import gc
from ml_utils import set_seed, train_model,train_test_split, get_model, evaluate_and_save, mask, run_feature_ablation,grid_search_model




def load_and_split_data(
    path="ml_processed",
    seed=42,
    split_ratio=(0.8, 0.1, 0.1),
    mode="across",
    target_pid=None,
    stride_seconds=2,
    window_seconds=20,
    sampling_rate=120
):
    set_seed(seed)

    # ---------- Load ----------
    X_array = np.load(f"{path}/X_array.npy")  # (N, T, C)
    y_array = np.load(f"{path}/y_array.npy")
    pid_array = np.load(f"{path}/pid_array.npy")
    feature_tag_list = np.load(f"{path}/feature_tag_list.npy").tolist()
    if X_array.shape[1] < X_array.shape[2]:  # í˜„ì¬ [N, C, T]ë¼ë©´
        X_array = X_array.transpose(0, 2, 1)  # â†’ [N, T, C]

    if mode == "across":
        unique_pids = np.unique(pid_array)
        np.random.default_rng(seed).shuffle(unique_pids)
        n = len(unique_pids)
        p_train = unique_pids[:int(n * split_ratio[0])]
        p_val   = unique_pids[int(n * split_ratio[0]):int(n * (split_ratio[0] + split_ratio[1]))]
        p_test  = unique_pids[int(n * (split_ratio[0] + split_ratio[1])):]

        X_train, y_train, pid_train = mask(X_array, y_array, pid_array, p_train)
        X_val, y_val, pid_val       = mask(X_array, y_array, pid_array, p_val)
        X_test, y_test, pid_test    = mask(X_array, y_array, pid_array, p_test)

        return (
            X_train, y_train, pid_train,
            X_val, y_val, pid_val,
            X_test, y_test, pid_test,
            feature_tag_list
        )

    elif mode == "within":
        if target_pid is None:
            raise ValueError("target_pid must be specified for within-participant mode.")

        # ---------- Pretrain from other participants ----------
        mask_pretrain = pid_array != target_pid
        mask_target   = pid_array == target_pid

        X_pre, y_pre = X_array[mask_pretrain], y_array[mask_pretrain]
        X_train, X_val, y_train, y_val = train_test_split(X_pre, y_pre, test_size=0.1, random_state=seed)
        pid_train = np.array(["pretrain"] * len(y_train))
        pid_val   = np.array(["val"] * len(y_val))

        # ---------- Fine-tune & Test split from target participant ----------
        X_target, y_target = X_array[mask_target], y_array[mask_target]

        cut = int(len(X_target) * 0.5)
        window_size = window_seconds * sampling_rate
        stride_size = stride_seconds * sampling_rate

        # ê²¹ì¹¨ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ fine-tune ë§ˆì§€ë§‰ windowì™€ test ì²« window ê°„ marginì„ ë‘ 
        overlap_margin = int(window_size // stride_size)

        X_finetune = X_target[:cut]
        y_finetune = y_target[:cut]

        X_test = X_target[cut + overlap_margin:]
        y_test = y_target[cut + overlap_margin:]
        pid_test = np.array([target_pid] * len(y_test))

        return (
            X_train, y_train, pid_train,
            X_val, y_val, pid_val,
            X_test, y_test, pid_test,
            X_finetune, y_finetune,
            feature_tag_list
        )

    else:
        raise ValueError(f"Invalid mode: {mode}. Choose from 'across' or 'within'.")




def run_ablation(X_train, y_train, pid_train, X_val, y_val, pid_val, feature_tag_list,
                 model_type="CNN", fixed_params=None, seed=42, num_epochs=10,
                 save_path="ablation_result.csv",
                 patience=10, min_delta=1e-6   # âœ… ì¶”ê°€
                 ):
    df_result = run_feature_ablation(
        X_train, y_train, pid_train,
        X_val, y_val, pid_val,
        feature_tags=feature_tag_list,
        model_type=model_type,
        fixed_params=fixed_params,
        num_epochs=num_epochs,
        seed=seed,
        patience=patience,        # âœ… ì „ë‹¬
        min_delta=min_delta       # âœ… ì „ë‹¬
    )
    df_result.to_csv(save_path, index=False)
    print(f"âœ… Ablation ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {save_path}")
    return df_result


def run_grid_search(
    X_train, y_train, pid_train,
    model_type, search_space,
    seed_list=(42, 43, 44),          # âœ… multi-seed ê¶Œì¥ ê¸°ë³¸
    num_epochs=20,
    use_internal_split=False,        # âœ… ì™¸ë¶€ val ê³ ì • ê¶Œì¥
    external_val_data=None,          # (X_val, y_val) í•„ìˆ˜ when False
    patience=10, min_delta=1e-6,
    **kwargs
):
    if use_internal_split is False and external_val_data is None:
        raise ValueError("use_internal_split=Falseë©´ external_val_data=(X_val, y_val)ì„ ë„˜ê²¨ì£¼ì„¸ìš”.")

    best_params, grid_results = grid_search_model(
        X_train, y_train, pid_train,
        model_type=model_type,
        search_space=search_space,
        num_epochs=num_epochs,
        seed_list=seed_list,                 # âœ… ì „ë‹¬
        use_internal_split=use_internal_split,
        external_val_data=external_val_data, # âœ… ì „ë‹¬
        patience=patience, min_delta=min_delta,
        **kwargs
    )
    print("âœ… Grid Search ì™„ë£Œ!")
    return best_params, grid_results


def train_and_evaluate_seeds(
    X_trainval, y_trainval, pid_trainval,
    X_test, y_test,
    model_type, best_params,
    device,
    num_seeds=10, num_epochs=20,
    patience=3, min_delta=1e-3
):
    """
    Seed ì•™ìƒë¸” í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ (CNN, GRU/LSTM ë“± ê³µìš©).
    - ì…ë ¥ í…ì„œ í˜•íƒœëŠ” (N, T, C) ê°€ì •.
    - CNN: params['input_channels'] = C ë¡œ ë³´ì •
    - RNNë¥˜(GRU/LSTM/Transformer): params['input_size'] = C ë¡œ ë³´ì •
    - train_model()ì€ ë‚´ë¶€ì—ì„œ best stateë¥¼ ì ìš©í•œ ëª¨ë¸ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •.

    Returns:
        all_train_losses: [seedë³„ train loss curve(list or np.array)]
        all_val_losses  : [seedë³„ val loss curve(list or np.array)]
        all_test_scores : [seedë³„ (r2, rmse, mae)]
    """
    import gc
    import torch
    from ml_utils import set_seed, get_model, evaluate_and_save, train_model

    # ---------- ê¸°ë³¸ ì²´í¬ ----------
    assert X_trainval.ndim == 3 and X_test.ndim == 3, "Expect inputs shaped (N, T, C)."
    assert len(X_trainval) == len(y_trainval) == len(pid_trainval), \
        "Length mismatch among X_trainval, y_trainval, pid_trainval."

    C_train = X_trainval.shape[-1]
    C_test  = X_test.shape[-1]
    mt = str(model_type).upper()

    # ---------- íŒŒë¼ë¯¸í„° ì£¼ì…(ë³´ì •) ----------
    # (ì˜ˆì „ì²˜ëŸ¼ 'ì‚¬ì „ assert'ë¡œ Noneì„ ê²€ì‚¬í•˜ì§€ ë§ê³ , ë¨¼ì € ì£¼ì… â†’ ì´í›„ ì¼ì¹˜ì„± ê²€ì‚¬)
    _best_params = dict(best_params)  # ì›ë³¸ ë³´ì¡´
    if mt == "CNN":
        if _best_params.get("input_channels") is None:
            _best_params["input_channels"] = C_train
        # ìµœì¢… ì¼ì¹˜ì„± í™•ì¸
        assert _best_params["input_channels"] == C_train, \
            f"[CNN] input_channels({ _best_params['input_channels'] }) != C_train({ C_train })"
        assert C_test == _best_params["input_channels"], \
            f"[CNN] C_test({ C_test }) must equal input_channels({ _best_params['input_channels'] })"
    else:
        # GRU / LSTM / Transformer ë“±
        if _best_params.get("input_size") is None:
            _best_params["input_size"] = C_train
        assert _best_params["input_size"] == C_train, \
            f"[RNN] input_size({ _best_params['input_size'] }) != C_train({ C_train })"
        assert C_test == _best_params["input_size"], \
            f"[RNN] C_test({ C_test }) must equal input_size({ _best_params['input_size'] })"

    # ---------- ë£¨í”„ ì¤€ë¹„ ----------
    all_train_losses, all_val_losses, all_test_scores = [], [], []

    for seed in range(num_seeds):
        print(f"\nğŸŸ¢ SEED {seed} ì‹œì‘\n")
        set_seed(seed)

        # ---- í•™ìŠµ (best stateê°€ ì ìš©ëœ model ë°˜í™˜ ê°€ì •) ----
        model, train_losses, val_losses, *_ = train_model(
            X_trainval, y_trainval,
            params=_best_params,
            model_type=model_type,
            num_epochs=num_epochs,
            seed=seed,
            pid_array=pid_trainval,
            return_curve=True,
            patience=patience,
            min_delta=min_delta
        )
        # ì»¤ë¸Œ ì €ì¥ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        all_train_losses.append(train_losses if train_losses is not None else [])
        all_val_losses.append(val_losses if val_losses is not None else [])

        # ---- í…ŒìŠ¤íŠ¸ìš© fresh ëª¨ë¸ ìƒì„± & ê°€ì¤‘ì¹˜ ë¡œë“œ ----
        if mt == "CNN":
            test_model = get_model(model_type, input_size=_best_params["input_channels"], params=_best_params).to(device)
        else:
            test_model = get_model(model_type, input_size=_best_params["input_size"], params=_best_params).to(device)

        test_model.load_state_dict(model.state_dict())

        # ---- í‰ê°€ & ì €ì¥ ----
        filename = f"{model_type.lower()}_test_predictions_seed{seed}.npz"
        test_r2, test_rmse, test_mae, _ = evaluate_and_save(
            test_model, (X_test, y_test), device, filename, model_type=model_type
        )
        all_test_scores.append((test_r2, test_rmse, test_mae))

        # ---- ì •ë¦¬ ----
        del model, test_model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    return all_train_losses, all_val_losses, all_test_scores



def summarize_test_results(all_test_scores):
    test_r2s = [r2 for r2, _, _ in all_test_scores]
    print(f"\nğŸ“Š í‰ê·  Test RÂ²: {np.mean(test_r2s):.4f} Â± {np.std(test_r2s):.4f}")
import numpy as np
import numpy as np
import pandas as pd
from typing import Sequence, Tuple, List, Union

def select_features_by_ablation(
    df_result: pd.DataFrame,
    feature_tag_list: Union[Sequence[str], np.ndarray, pd.Series],
    top_k: int = None,
    threshold: float = None,
    strict: bool = True,         # True: ëˆ„ë½ íƒœê·¸ ë°œê²¬ ì‹œ ì—ëŸ¬, False: ì¡°ìš©íˆ ìŠ¤í‚µ
    allow_duplicates: bool = True  # True: ë™ì¼ ì´ë¦„ ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©(ê²½ê³ ), False: ì—ëŸ¬
) -> Tuple[List[str], List[int]]:
    """
    Ablation ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” featureë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

    Parameters:
    - df_result: run_feature_ablation ê²°ê³¼(ì—´: ['feature_removed','val_r2'] í¬í•¨)
    - feature_tag_list: ì „ì²´ feature ì´ë¦„ ì‹œí€€ìŠ¤(list/ndarray/Series ëª¨ë‘ OK)
    - top_k: ì„ íƒí•  feature ìˆ˜
    - threshold: drop_in_r2 ê¸°ì¤€ê°’ (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)
    - strict: ì„ íƒëœ featureê°€ tag ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì—ëŸ¬(True) ë˜ëŠ” ìŠ¤í‚µ(False)
    - allow_duplicates: feature_tag_list ë‚´ ì¤‘ë³µ ì´ë¦„ í—ˆìš© ì—¬ë¶€

    Returns:
    - selected_features: ì¤‘ìš” feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    - selected_indices:  ì¤‘ìš” feature ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸(ì±„ë„ ì°¨ì› ìŠ¬ë¼ì´ìŠ¤ìš©)
    """
    # ---------- íƒ€ì… ë°©ì–´ ----------
    if isinstance(feature_tag_list, (np.ndarray, pd.Series, tuple)):
        feature_tag_list = list(feature_tag_list)
    elif not isinstance(feature_tag_list, list):
        feature_tag_list = list(feature_tag_list)

    # ---------- ê¸°ë³¸ ê²€ì¦ ----------
    required_cols = {"feature_removed", "val_r2"}
    missing_cols = required_cols - set(df_result.columns)
    if missing_cols:
        raise ValueError(f"âŒ df_resultì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {sorted(missing_cols)}")

    if "None (baseline)" not in set(df_result["feature_removed"]):
        raise ValueError("âŒ Ablation ê²°ê³¼ì— 'None (baseline)' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ---------- Drop-in RÂ² ê³„ì‚° ----------
    baseline_r2 = df_result.loc[df_result["feature_removed"] == "None (baseline)", "val_r2"].iloc[0]
    ablation_only = df_result.loc[df_result["feature_removed"] != "None (baseline)"].copy()
    ablation_only["drop_in_r2"] = baseline_r2 - ablation_only["val_r2"]

    # ì¤‘ìš”ë„ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬ (drop_in_r2ê°€ í´ìˆ˜ë¡ ì œê±° ì‹œ ì„±ëŠ¥ í•˜ë½ â†’ ì›ë˜ ì¤‘ìš”)
    ablation_only = ablation_only.sort_values("drop_in_r2", ascending=False)

    # ---------- ì„ íƒ ê·œì¹™ ----------
    if top_k is not None:
        selected = ablation_only.head(top_k)
    elif threshold is not None:
        selected = ablation_only[ablation_only["drop_in_r2"] >= threshold]
    else:
        selected = ablation_only

    selected_features = selected["feature_removed"].tolist()

    # ---------- ì¸ë±ìŠ¤ ë§¤í•‘(ì•ˆì „/ê³ ì†) ----------
    # ë™ì¼ ì´ë¦„ ì¤‘ë³µ ì—¬ë¶€ ì²´í¬
    name2idxs = {}
    for i, name in enumerate(feature_tag_list):
        name2idxs.setdefault(name, []).append(i)

    if not allow_duplicates:
        dups = {k: v for k, v in name2idxs.items() if len(v) > 1}
        if dups:
            sample = {k: v[:3] for k, v in dups.items()}
            raise ValueError(f"âŒ feature_tag_listì— ì¤‘ë³µ ì´ë¦„ì´ ìˆìŠµë‹ˆë‹¤(allow_duplicates=False): {sample}")

    selected_indices = []
    missing = []
    for f in selected_features:
        if f not in name2idxs:
            missing.append(f)
            if not strict:
                continue
        else:
            # ì¤‘ë³µì´ë©´ ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ ì‚¬ìš© (ê²½ìš°ì— ë”°ë¼ ì •ì±… ë³€ê²½ ê°€ëŠ¥)
            selected_indices.append(name2idxs[f][0])

    if missing and strict:
        raise ValueError(f"âŒ ì„ íƒëœ featureê°€ feature_tag_listì— ì—†ìŠµë‹ˆë‹¤: {missing}")
    elif missing and not strict:
        print(f"âš ï¸ ë‹¤ìŒ featureëŠ” tag ë¦¬ìŠ¤íŠ¸ì— ì—†ì–´ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤: {missing}")

    # ---------- ë¡œê·¸ ----------
    print(f"ğŸ“Œ ì„ íƒëœ feature ìˆ˜: {len(selected_indices)} / {len(feature_tag_list)}")
    print(f"ğŸ“Œ feature_indices: {selected_indices}")

    return [feature_tag_list[i] for i in selected_indices], selected_indices


