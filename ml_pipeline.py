import numpy as np
import torch
import gc
from ml_utils import set_seed, train_model,train_test_split, get_model, evaluate_and_save, mask, run_feature_ablation,grid_search_model




def load_and_split_data(
    path="ml_processed",
    seed=42,
    split_ratio=(0.8, 0.1, 0.1),
    mode="across",
    target_pid=None,
    stride_seconds=2,
    window_seconds=20,
    sampling_rate=120
):
    set_seed(seed)

    # ---------- Load ----------
    X_array = np.load(f"{path}/X_array.npy")  # (N, T, C)
    y_array = np.load(f"{path}/y_array.npy")
    pid_array = np.load(f"{path}/pid_array.npy")
    feature_tag_list = np.load(f"{path}/feature_tag_list.npy").tolist()
    if X_array.shape[1] < X_array.shape[2]:  # í˜„ì¬ [N, C, T]ë¼ë©´
        X_array = X_array.transpose(0, 2, 1)  # â†’ [N, T, C]

    if mode == "across":
        unique_pids = np.unique(pid_array)
        np.random.default_rng(seed).shuffle(unique_pids)
        n = len(unique_pids)
        p_train = unique_pids[:int(n * split_ratio[0])]
        p_val   = unique_pids[int(n * split_ratio[0]):int(n * (split_ratio[0] + split_ratio[1]))]
        p_test  = unique_pids[int(n * (split_ratio[0] + split_ratio[1])):]

        X_train, y_train, pid_train = mask(X_array, y_array, pid_array, p_train)
        X_val, y_val, pid_val       = mask(X_array, y_array, pid_array, p_val)
        X_test, y_test, pid_test    = mask(X_array, y_array, pid_array, p_test)

        return (
            X_train, y_train, pid_train,
            X_val, y_val, pid_val,
            X_test, y_test, pid_test,
            feature_tag_list
        )

    elif mode == "within":
        if target_pid is None:
            raise ValueError("target_pid must be specified for within-participant mode.")

        # ---------- Pretrain from other participants ----------
        mask_pretrain = pid_array != target_pid
        mask_target   = pid_array == target_pid

        X_pre, y_pre = X_array[mask_pretrain], y_array[mask_pretrain]
        X_train, X_val, y_train, y_val = train_test_split(X_pre, y_pre, test_size=0.1, random_state=seed)
        pid_train = np.array(["pretrain"] * len(y_train))
        pid_val   = np.array(["val"] * len(y_val))

        # ---------- Fine-tune & Test split from target participant ----------
        X_target, y_target = X_array[mask_target], y_array[mask_target]

        cut = int(len(X_target) * 0.5)
        window_size = window_seconds * sampling_rate
        stride_size = stride_seconds * sampling_rate

        # ê²¹ì¹¨ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ fine-tune ë§ˆì§€ë§‰ windowì™€ test ì²« window ê°„ marginì„ ë‘ 
        overlap_margin = int(window_size // stride_size)

        X_finetune = X_target[:cut]
        y_finetune = y_target[:cut]

        X_test = X_target[cut + overlap_margin:]
        y_test = y_target[cut + overlap_margin:]
        pid_test = np.array([target_pid] * len(y_test))

        return (
            X_train, y_train, pid_train,
            X_val, y_val, pid_val,
            X_test, y_test, pid_test,
            X_finetune, y_finetune,
            feature_tag_list
        )

    else:
        raise ValueError(f"Invalid mode: {mode}. Choose from 'across' or 'within'.")




def run_ablation(X_train, y_train, pid_train, X_val, y_val, pid_val, feature_tag_list,
                 model_type="CNN", fixed_params=None, seed=42, num_epochs=10,
                 save_path="ablation_result.csv",
                 patience=10, min_delta=1e-6, criterion=torch.nn.MSELoss(reduction="mean")
                 ):
    df_result = run_feature_ablation(
        X_train, y_train, pid_train,
        X_val, y_val, pid_val,
        feature_tags=feature_tag_list,
        model_type=model_type,
        fixed_params=fixed_params,
        num_epochs=num_epochs,
        seed=seed,
        patience=patience,        # âœ… ì „ë‹¬
        min_delta=min_delta,       # âœ… ì „ë‹¬
        criterion=criterion  # âœ… ì¶”ê°€
    )
    df_result.to_csv(save_path, index=False)
    print(f"âœ… Ablation ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {save_path}")
    return df_result


def run_grid_search(
    X_train, y_train, pid_train,
    model_type, search_space,
    seed_list=(42, 43, 44),          # âœ… multi-seed ê¶Œì¥ ê¸°ë³¸
    num_epochs=20,
    use_internal_split=False,        # âœ… ì™¸ë¶€ val ê³ ì • ê¶Œì¥
    external_val_data=None,          # (X_val, y_val) í•„ìˆ˜ when False
    patience=10, min_delta=1e-6, criterion=torch.nn.MSELoss(reduction="mean"),
    **kwargs
):
    if use_internal_split is False and external_val_data is None:
        raise ValueError("use_internal_split=Falseë©´ external_val_data=(X_val, y_val)ì„ ë„˜ê²¨ì£¼ì„¸ìš”.")

    best_params, grid_results = grid_search_model(
        X_train, y_train, pid_train,
        model_type=model_type,
        search_space=search_space,
        num_epochs=num_epochs,
        seed_list=seed_list,                 # âœ… ì „ë‹¬
        use_internal_split=use_internal_split,
        external_val_data=external_val_data, # âœ… ì „ë‹¬
        patience=patience, min_delta=min_delta, criterion=criterion,
        **kwargs
    )
    print("âœ… Grid Search ì™„ë£Œ!")
    return best_params, grid_results


# def train_and_evaluate_seeds(
#     X_trainval, y_trainval, pid_trainval,
#     X_test, y_test,
#     model_type, best_params,
#     device,
#     num_seeds=10, num_epochs=20,
#     patience=3, min_delta=1e-3,
#     use_internal_split=True,             # âœ… ì¶”ê°€
#     external_val_data=None,              # âœ… (X_val, y_val) íŠœí”Œ ë˜ëŠ” None
#     deterministic=True,                   # âœ… ì„ íƒ: ê²°ì •ì„± ì œì–´
#     criterion=torch.nn.MSELoss(reduction="mean"),
#     internal_split_mode="train_val",      # {"train_val","train_only","train_val_test"}
#     internal_val_ratio=0.20,
# ):
#     """
#     Seed ì•™ìƒë¸” í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ (CNN, GRU/LSTM ë“± ê³µìš©).
#     - ì…ë ¥ í…ì„œ í˜•íƒœëŠ” (N, T, C) ê°€ì •.
#     - CNN: params['input_channels'] = C ë¡œ ë³´ì •
#     - RNNë¥˜(GRU/LSTM/Transformer): params['input_size'] = C ë¡œ ë³´ì •
#     - train_model()ì€ ë‚´ë¶€ì—ì„œ best stateë¥¼ ì ìš©í•œ ëª¨ë¸ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •.

#     Returns:
#         all_train_losses: [seedë³„ train loss curve(list or np.array)]
#         all_val_losses  : [seedë³„ val loss curve(list or np.array)]
#         all_test_scores : [seedë³„ (r2, rmse, mae)]
#     """
#     import gc
#     import torch
#     from ml_utils import set_seed, get_model, evaluate_and_save, train_model

#     # ---------- ê¸°ë³¸ ì²´í¬ ----------
#     assert X_trainval.ndim == 3 and X_test.ndim == 3, "Expect inputs shaped (N, T, C)."
#     assert len(X_trainval) == len(y_trainval) == len(pid_trainval), \
#         "Length mismatch among X_trainval, y_trainval, pid_trainval."

#     C_train = X_trainval.shape[-1]
#     C_test  = X_test.shape[-1]
#     mt = str(model_type).upper()

#     # ---------- íŒŒë¼ë¯¸í„° ì£¼ì…(ë³´ì •) ----------
#     # (ì˜ˆì „ì²˜ëŸ¼ 'ì‚¬ì „ assert'ë¡œ Noneì„ ê²€ì‚¬í•˜ì§€ ë§ê³ , ë¨¼ì € ì£¼ì… â†’ ì´í›„ ì¼ì¹˜ì„± ê²€ì‚¬)
#     _best_params = dict(best_params)  # ì›ë³¸ ë³´ì¡´
#     if mt == "CNN":
#         if _best_params.get("input_channels") is None:
#             _best_params["input_channels"] = C_train
#         # ìµœì¢… ì¼ì¹˜ì„± í™•ì¸
#         assert _best_params["input_channels"] == C_train, \
#             f"[CNN] input_channels({ _best_params['input_channels'] }) != C_train({ C_train })"
#         assert C_test == _best_params["input_channels"], \
#             f"[CNN] C_test({ C_test }) must equal input_channels({ _best_params['input_channels'] })"
#     else:
#         # GRU / LSTM / Transformer ë“±
#         if _best_params.get("input_size") is None:
#             _best_params["input_size"] = C_train
#         assert _best_params["input_size"] == C_train, \
#             f"[RNN] input_size({ _best_params['input_size'] }) != C_train({ C_train })"
#         assert C_test == _best_params["input_size"], \
#             f"[RNN] C_test({ C_test }) must equal input_size({ _best_params['input_size'] })"

#     # ---------- ë£¨í”„ ì¤€ë¹„ ----------
#     all_train_losses, all_val_losses, all_test_scores = [], [], []
#     all_train_scores, all_val_scores = [], []

#     for seed in range(num_seeds):
#         print(f"\nğŸŸ¢ SEED {seed} ì‹œì‘\n")
#         set_seed(seed)

#         # ---- í•™ìŠµ (best stateê°€ ì ìš©ëœ model ë°˜í™˜ ê°€ì •) ----
#         model, train_losses, val_losses, val_r2, val_rmse, val_mae, train_idx, val_idx, train_r2, train_rmse, train_mae = train_model(
#             X_trainval, y_trainval,
#             params=_best_params,
#             model_type=model_type,
#             num_epochs=num_epochs,
#             seed=seed,
#             pid_array=pid_trainval,
#             return_curve=True,
#             patience=patience,
#             min_delta=min_delta,
#             use_internal_split=use_internal_split,     # âœ… ë°”ê¹¥ ì¸ì ê·¸ëŒ€ë¡œ ì „ë‹¬
#             external_val_data=external_val_data,       # âœ… ë°”ê¹¥ ì¸ì ê·¸ëŒ€ë¡œ ì „ë‹¬
#             deterministic=deterministic,                # âœ… ì„ íƒ
#             criterion=criterion,                          # âœ… ì¶”ê°€
#             internal_split_mode=internal_split_mode,      # âœ… ë‚´ë¶€ ëª¨ë“œ/ë¹„ìœ¨ ì „ë‹¬
#             internal_val_ratio=internal_val_ratio,
#         )
#         # ì»¤ë¸Œ ì €ì¥ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
#         all_train_losses.append(train_losses if train_losses is not None else [])
#         all_val_losses.append(val_losses if val_losses is not None else [])
#         all_train_scores.append((float(train_r2), float(train_rmse), float(train_mae)))
#         all_val_scores.append((float(val_r2), float(val_rmse), float(val_mae)))

#         # ---- í…ŒìŠ¤íŠ¸ìš© fresh ëª¨ë¸ ìƒì„± & ê°€ì¤‘ì¹˜ ë¡œë“œ ----
#         if mt == "CNN":
#             test_model = get_model(model_type, input_size=_best_params["input_channels"], params=_best_params).to(device)
#         else:
#             test_model = get_model(model_type, input_size=_best_params["input_size"], params=_best_params).to(device)

#         test_model.load_state_dict(model.state_dict())

#         # ---- í‰ê°€ & ì €ì¥ ----
#         filename = f"{model_type.lower()}_test_predictions_seed{seed}.npz"
#         test_r2, test_rmse, test_mae, _ = evaluate_and_save(
#             test_model, (X_test, y_test), device, filename, model_type=model_type
#         )
#         all_test_scores.append((test_r2, test_rmse, test_mae))

#         # ---- ì •ë¦¬ ----
#         del model, test_model
#         if torch.cuda.is_available():
#             torch.cuda.empty_cache()
#         gc.collect()

#     return all_train_losses, all_val_losses, all_test_scores, all_train_scores, all_val_scores

def train_and_evaluate_seeds(
    X_trainval, y_trainval, pid_trainval,
    X_test, y_test,
    model_type, best_params,
    device,
    num_seeds=10, num_epochs=20,
    patience=3, min_delta=1e-3,
    use_internal_split=True,
    external_val_data=None,              # (X_val, y_val) ë˜ëŠ” (X_val, y_val, pid_val)
    deterministic=True,
    criterion=torch.nn.MSELoss(reduction="mean"),
    internal_split_mode="train_val",      # {"train_val","train_only","train_val_test","two_stage"} ë“±
    internal_val_ratio=0.20,
):
    """
    Seed ì•™ìƒë¸” í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ (CNN, GRU/LSTM ë“± ê³µìš©).

    âœ… ì¶”ê°€ ë™ì‘ (ì‹œê·¸ë‹ˆì²˜ ë³€ê²½ ì—†ì´):
      - external_val_dataê°€ (X_val, y_val, pid_val) í˜•íƒœë©´,
        Stage-1: TRAIN í•™ìŠµ + external VALë¡œ best_epoch ê²°ì •
        Stage-2: TRAIN+VAL ì „ì²´ë¡œ best_epochë§Œí¼ ê²€ì¦ ì—†ì´ ì¬í•™ìŠµ(final retrain)
        â†’ ëˆ„ìˆ˜ ì—†ì´ ì •ì„ final modelë¡œ TEST í‰ê°€

      - external_val_dataê°€ (X_val, y_val)ë§Œ ì£¼ì–´ì§€ë©´ ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ ë™ì‘(=Stage-2 ìŠ¤í‚µ).
    """
    import gc
    import numpy as np
    import torch
    from ml_utils import set_seed, get_model, evaluate_and_save, train_model

    # ---------- ê¸°ë³¸ ì²´í¬ ----------
    assert X_trainval.ndim == 3 and X_test.ndim == 3, "Expect inputs shaped (N, T, C)."
    assert len(X_trainval) == len(y_trainval) == len(pid_trainval), \
        "Length mismatch among X_trainval, y_trainval, pid_trainval."

    C_train = X_trainval.shape[-1]
    C_test  = X_test.shape[-1]
    mt = str(model_type).upper()

    # ---------- external_val_data íŒŒì‹± (ì‹œê·¸ë‹ˆì²˜ ìœ ì§€) ----------
    # í—ˆìš©:
    #   - None
    #   - (X_val, y_val)
    #   - (X_val, y_val, pid_val)  -> final retrain ë°œë™ìš©
    pid_val_ext = None
    external_val_xy = external_val_data

    if external_val_data is not None and isinstance(external_val_data, (tuple, list)):
        if len(external_val_data) == 3:
            X_val_ext, y_val_ext, pid_val_ext = external_val_data
            external_val_xy = (X_val_ext, y_val_ext)
        elif len(external_val_data) == 2:
            X_val_ext, y_val_ext = external_val_data
            external_val_xy = (X_val_ext, y_val_ext)
        else:
            raise ValueError("external_val_data must be None, (X_val,y_val), or (X_val,y_val,pid_val).")
    else:
        X_val_ext = y_val_ext = None  # not used unless tuple/list

    # ---------- íŒŒë¼ë¯¸í„° ì£¼ì…(ë³´ì •) ----------
    _best_params = dict(best_params)  # ì›ë³¸ ë³´ì¡´
    if mt == "CNN":
        if _best_params.get("input_channels") is None:
            _best_params["input_channels"] = C_train
        assert _best_params["input_channels"] == C_train, \
            f"[CNN] input_channels({_best_params['input_channels']}) != C_train({C_train})"
        assert C_test == _best_params["input_channels"], \
            f"[CNN] C_test({C_test}) must equal input_channels({_best_params['input_channels']})"
    else:
        if _best_params.get("input_size") is None:
            _best_params["input_size"] = C_train
        assert _best_params["input_size"] == C_train, \
            f"[RNN] input_size({_best_params['input_size']}) != C_train({C_train})"
        assert C_test == _best_params["input_size"], \
            f"[RNN] C_test({C_test}) must equal input_size({_best_params['input_size']})"

    # ---------- ë£¨í”„ ì¤€ë¹„ ----------
    all_train_losses, all_val_losses, all_test_scores = [], [], []
    all_train_scores, all_val_scores = [], []

    for seed in range(num_seeds):
        print(f"\nğŸŸ¢ SEED {seed} ì‹œì‘\n")
        set_seed(seed)

        # ==========================
        # Stage-1: TRAIN í•™ìŠµ + external VALë¡œ best_state ì„ íƒ
        # ==========================
        model, train_losses, val_losses, val_r2, val_rmse, val_mae, train_idx, val_idx, train_r2, train_rmse, train_mae = train_model(
            X_trainval, y_trainval,
            params=_best_params,
            model_type=model_type,
            num_epochs=num_epochs,
            seed=seed,
            pid_array=pid_trainval,
            return_curve=True,
            patience=patience,
            min_delta=min_delta,
            use_internal_split=use_internal_split,
            external_val_data=external_val_xy,   # âœ… í•­ìƒ (X_val,y_val) í˜•íƒœë¡œë§Œ ì „ë‹¬
            deterministic=deterministic,
            criterion=criterion,
            internal_split_mode=internal_split_mode,
            internal_val_ratio=internal_val_ratio,
        )

        # ì»¤ë¸Œ/ì ìˆ˜ ì €ì¥
        all_train_losses.append(train_losses if train_losses is not None else [])
        all_val_losses.append(val_losses if val_losses is not None else [])
        all_train_scores.append((float(train_r2), float(train_rmse), float(train_mae)))
        all_val_scores.append((float(val_r2), float(val_rmse), float(val_mae)))

        # ==========================
        # Stage-2 (ì˜µì…˜): TRAIN+VALë¡œ "best_epochë§Œí¼" ê²€ì¦ ì—†ì´ final retrain
        #  - pid_val_extê°€ ì œê³µëœ ê²½ìš°ë§Œ ë°œë™
        # ==========================
        if (pid_val_ext is not None) and (X_val_ext is not None) and (y_val_ext is not None):
            # best_epochë¥¼ val_lossesì˜ ìµœì†Œ ì§€ì ìœ¼ë¡œ ì¶”ì • (Stage-1 ê¸°ì¤€)
            if val_losses is not None and len(val_losses) > 0:
                best_epoch_ext = int(np.argmin(np.asarray(val_losses)) + 1)
            else:
                # fallback: train_losses ê¸¸ì´ or num_epochs
                best_epoch_ext = int(len(train_losses) if train_losses is not None and len(train_losses) > 0 else num_epochs)
            best_epoch_ext = max(1, best_epoch_ext)

            X_final = np.concatenate([X_trainval, X_val_ext], axis=0)
            y_final = np.concatenate([y_trainval, y_val_ext], axis=0)
            pid_final = np.concatenate([pid_trainval, pid_val_ext], axis=0)

            # ê²€ì¦ ì—†ì´ ê³ ì • epochë§Œí¼ ì¬í•™ìŠµ (ëˆ„ìˆ˜ ì—†ìŒ)
            model_final, *_ = train_model(
                X_final, y_final,
                params=_best_params,
                model_type=model_type,
                num_epochs=best_epoch_ext,
                seed=seed,
                pid_array=pid_final,
                return_curve=False,
                use_internal_split=True,
                external_val_data=None,
                # train_onlyë¼ early stop ê°œë…ì´ ì—†ì§€ë§Œ ì•ˆì „í•˜ê²Œ í¬ê²Œ ë‘ 
                patience=999999,
                min_delta=0.0,
                deterministic=deterministic,
                criterion=criterion,
                internal_split_mode="train_only",
                internal_val_ratio=0.0,
            )
            model_to_test = model_final
            print(f"[FINAL] retrained on (train+val) for {best_epoch_ext} epochs (no-val).")
        else:
            model_to_test = model  # ê¸°ì¡´ ë™ì‘ ìœ ì§€

        # ---- í…ŒìŠ¤íŠ¸ìš© fresh ëª¨ë¸ ìƒì„± & ê°€ì¤‘ì¹˜ ë¡œë“œ ----
        if mt == "CNN":
            test_model = get_model(model_type, input_size=_best_params["input_channels"], params=_best_params).to(device)
        else:
            test_model = get_model(model_type, input_size=_best_params["input_size"], params=_best_params).to(device)

        test_model.load_state_dict(model_to_test.state_dict())

        # ---- í‰ê°€ & ì €ì¥ ----
        filename = f"{model_type.lower()}_test_predictions_seed{seed}.npz"
        test_r2, test_rmse, test_mae, _ = evaluate_and_save(
            test_model, (X_test, y_test), device, filename, model_type=model_type
        )
        all_test_scores.append((test_r2, test_rmse, test_mae))

        # ---- ì •ë¦¬ ----
        del model, test_model
        if 'model_final' in locals():
            try:
                del model_final
            except Exception:
                pass
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    return all_train_losses, all_val_losses, all_test_scores, all_train_scores, all_val_scores


def summarize_test_results(all_test_scores):
    test_r2s = [r2 for r2, _, _ in all_test_scores]
    print(f"\nğŸ“Š í‰ê·  Test RÂ²: {np.mean(test_r2s):.4f} Â± {np.std(test_r2s):.4f}")
import numpy as np
import numpy as np
import pandas as pd
from typing import Sequence, Tuple, List, Union

def select_features_by_ablation(
    df_result: pd.DataFrame,
    feature_tag_list: Union[Sequence[str], np.ndarray, pd.Series],
    top_k: int = None,
    threshold: float = None,
    strict: bool = True,         # True: ëˆ„ë½ íƒœê·¸ ë°œê²¬ ì‹œ ì—ëŸ¬, False: ì¡°ìš©íˆ ìŠ¤í‚µ
    allow_duplicates: bool = True  # True: ë™ì¼ ì´ë¦„ ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©(ê²½ê³ ), False: ì—ëŸ¬
) -> Tuple[List[str], List[int]]:
    """
    Ablation ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” featureë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

    Parameters:
    - df_result: run_feature_ablation ê²°ê³¼(ì—´: ['feature_removed','val_r2'] í¬í•¨)
    - feature_tag_list: ì „ì²´ feature ì´ë¦„ ì‹œí€€ìŠ¤(list/ndarray/Series ëª¨ë‘ OK)
    - top_k: ì„ íƒí•  feature ìˆ˜
    - threshold: drop_in_r2 ê¸°ì¤€ê°’ (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)
    - strict: ì„ íƒëœ featureê°€ tag ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì—ëŸ¬(True) ë˜ëŠ” ìŠ¤í‚µ(False)
    - allow_duplicates: feature_tag_list ë‚´ ì¤‘ë³µ ì´ë¦„ í—ˆìš© ì—¬ë¶€

    Returns:
    - selected_features: ì¤‘ìš” feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    - selected_indices:  ì¤‘ìš” feature ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸(ì±„ë„ ì°¨ì› ìŠ¬ë¼ì´ìŠ¤ìš©)
    """
    # ---------- íƒ€ì… ë°©ì–´ ----------
    if isinstance(feature_tag_list, (np.ndarray, pd.Series, tuple)):
        feature_tag_list = list(feature_tag_list)
    elif not isinstance(feature_tag_list, list):
        feature_tag_list = list(feature_tag_list)

    # ---------- ê¸°ë³¸ ê²€ì¦ ----------
    required_cols = {"feature_removed", "val_r2"}
    missing_cols = required_cols - set(df_result.columns)
    if missing_cols:
        raise ValueError(f"âŒ df_resultì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {sorted(missing_cols)}")

    if "None (baseline)" not in set(df_result["feature_removed"]):
        raise ValueError("âŒ Ablation ê²°ê³¼ì— 'None (baseline)' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ---------- Drop-in RÂ² ê³„ì‚° ----------
    baseline_r2 = df_result.loc[df_result["feature_removed"] == "None (baseline)", "val_r2"].iloc[0]
    ablation_only = df_result.loc[df_result["feature_removed"] != "None (baseline)"].copy()
    ablation_only["drop_in_r2"] = baseline_r2 - ablation_only["val_r2"]

    # ì¤‘ìš”ë„ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬ (drop_in_r2ê°€ í´ìˆ˜ë¡ ì œê±° ì‹œ ì„±ëŠ¥ í•˜ë½ â†’ ì›ë˜ ì¤‘ìš”)
    ablation_only = ablation_only.sort_values("drop_in_r2", ascending=False)

    # ---------- ì„ íƒ ê·œì¹™ ----------
    if top_k is not None:
        selected = ablation_only.head(top_k)
    elif threshold is not None:
        selected = ablation_only[ablation_only["drop_in_r2"] >= threshold]
    else:
        selected = ablation_only

    selected_features = selected["feature_removed"].tolist()

    # ---------- ì¸ë±ìŠ¤ ë§¤í•‘(ì•ˆì „/ê³ ì†) ----------
    # ë™ì¼ ì´ë¦„ ì¤‘ë³µ ì—¬ë¶€ ì²´í¬
    name2idxs = {}
    for i, name in enumerate(feature_tag_list):
        name2idxs.setdefault(name, []).append(i)

    if not allow_duplicates:
        dups = {k: v for k, v in name2idxs.items() if len(v) > 1}
        if dups:
            sample = {k: v[:3] for k, v in dups.items()}
            raise ValueError(f"âŒ feature_tag_listì— ì¤‘ë³µ ì´ë¦„ì´ ìˆìŠµë‹ˆë‹¤(allow_duplicates=False): {sample}")

    selected_indices = []
    missing = []
    for f in selected_features:
        if f not in name2idxs:
            missing.append(f)
            if not strict:
                continue
        else:
            # ì¤‘ë³µì´ë©´ ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ ì‚¬ìš© (ê²½ìš°ì— ë”°ë¼ ì •ì±… ë³€ê²½ ê°€ëŠ¥)
            selected_indices.append(name2idxs[f][0])

    if missing and strict:
        raise ValueError(f"âŒ ì„ íƒëœ featureê°€ feature_tag_listì— ì—†ìŠµë‹ˆë‹¤: {missing}")
    elif missing and not strict:
        print(f"âš ï¸ ë‹¤ìŒ featureëŠ” tag ë¦¬ìŠ¤íŠ¸ì— ì—†ì–´ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤: {missing}")

    # ---------- ë¡œê·¸ ----------
    print(f"ğŸ“Œ ì„ íƒëœ feature ìˆ˜: {len(selected_indices)} / {len(feature_tag_list)}")
    print(f"ğŸ“Œ feature_indices: {selected_indices}")

    return [feature_tag_list[i] for i in selected_indices], selected_indices


# -*- coding: utf-8 -*-
import os
import gc
import json

import numpy as np
import pandas as pd
import torch

from ml_utils import (
    to_NTC_strict,
    hv_mask_from_train_x,
    hv_mask_from_train_y,
    center_from_train_split,
    _fit_scene_stats,
    _transform_scenewise,
)

from ml_pipeline import train_and_evaluate_seeds  # ì´ë¯¸ ê°™ì€ íŒŒì¼ì´ë©´ ì´ ì„í¬íŠ¸ëŠ” ë¹¼ë„ ë¨


def run_lopo_cnn(
    data_dir: str,
    out_dir: str,
    best_cnn_params: dict,
    *,
    hv_mode: str = "y_train",        # {"none","y_train","x_variance"}
    hv_quantile: float = 0.3,
    num_seeds: int = 5,
    base_seed: int = 42,
    num_epochs: int = 50,
    internal_val_ratio: float = 0.2,
    patience: int = 7,
    min_delta: float = 1e-3,
    device: torch.device | None = None,
):
    """
    Leave-One-Participant-Out (LOPO) for CNN (regression ì „ìš©).

    - í•œ ë²ˆë§Œ X/y/pid/scene/feature_tag_list ë¡œë“œ
    - ê° PIDë¥¼ testë¡œ ë‘ê³ , ë‚˜ë¨¸ì§€ë¥¼ train(+internal val)ìœ¼ë¡œ ì‚¬ìš©
    - HV mask, y-centering, scene-wise zscoreëŠ” foldë§ˆë‹¤
      "train subset" ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ê³„ì‚° (leakage-free)
    - CNN hyperparameterëŠ” best_cnn_paramsë¡œ ê³ ì •, seedë§Œ ì—¬ëŸ¬ ê°œ ë°˜ë³µ

    data_dirì—ëŠ” ë‹¤ìŒ íŒŒì¼ì´ ìˆë‹¤ê³  ê°€ì •:
        X_array.npy           # (N,T,C) ë˜ëŠ” (N,C,T)
        y_array.npy           # (N,)
        pid_array.npy         # (N,)
        scene_array.npy       # (N,)
        windex_array.npy      # (N,)   â† ì—¬ê¸°ì„œëŠ” ì§ì ‘ ì•ˆ ì¨ë„ ë¨ (ì˜µì…˜)
        feature_tag_list.npy  # list[str], ê¸¸ì´ = C

    out_dirì—ëŠ”:
        lopo_detail_cnn.csv   # (target_pid Ã— seed) ë‹¨ìœ„ ì ìˆ˜
        lopo_summary_cnn.csv  # PIDë³„ meanÂ±std (r2, rmse, mae)
        meta_lopo_cnn.json    # ì „ì²´ ì„¤ì • ë° ìš”ì•½
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    os.makedirs(out_dir, exist_ok=True)
    print(f"[LOPO] data_dir={data_dir}")
    print(f"[LOPO] out_dir={out_dir}")
    print(f"[LOPO] device={device}")

    # ----------------------------------------------------
    # 1) ë°ì´í„° ë¡œë“œ
    # ----------------------------------------------------
    X = np.load(os.path.join(data_dir, "X_array.npy"))          # (N,*,*)
    y = np.load(os.path.join(data_dir, "y_array.npy")).astype(np.float32)
    pid = np.load(os.path.join(data_dir, "pid_array.npy"))
    scene = np.load(os.path.join(data_dir, "scene_array.npy"))
    widx = np.load(os.path.join(data_dir, "windex_array.npy"))

    feature_tag_list = np.load(
        os.path.join(data_dir, "feature_tag_list.npy"),
        allow_pickle=True,
    ).tolist()

    # (N,C,T)/(N,T,C) â†’ (N,T,C)
    X = to_NTC_strict(X, feature_tag_list)
    N, T, C = X.shape
    print(f"[LOPO] Loaded X: (N,T,C)=({N},{T},{C}), y={y.shape}, pid={pid.shape}")

    # NaN / Inf ì œê±° (X ë˜ëŠ” yì— í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í•´ë‹¹ ìœˆë„ìš° ì œê±°)
    finite_mask = np.isfinite(X).all(axis=(1, 2)) & np.isfinite(y)
    num_bad = int((~finite_mask).sum())
    if num_bad > 0:
        print(f"[CLEAN] Dropping {num_bad} / {len(finite_mask)} windows with NaN/Inf in X or y")
        X = X[finite_mask]
        y = y[finite_mask]
        pid = pid[finite_mask]
        scene = scene[finite_mask]
        widx = widx[finite_mask]
    else:
        print("[CLEAN] No NaN/Inf detected in X/y after load")

    # PID ëª©ë¡
    unique_pids = np.unique(pid)
    print(f"[LOPO] Unique PIDs: {len(unique_pids)} participants")

    # ----------------------------------------------------
    # 2) CNN íŒŒë¼ë¯¸í„° ê³ ì • ì„¸íŒ…
    # ----------------------------------------------------
    base_params = dict(best_cnn_params)   # ì›ë³¸ ë³´í˜¸
    base_params["input_channels"] = C     # CNNì€ ì±„ë„ ìˆ˜ í•„ìš” (N,T,C â†’ C)
    # batch_sizeê°€ best_cnn_params ì•ˆì— ì´ë¯¸ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©

    # seed ë¦¬ìŠ¤íŠ¸
    seed_list = [base_seed + i for i in range(num_seeds)]

    # ê²°ê³¼ë¥¼ ìŒ“ì„ ë¦¬ìŠ¤íŠ¸
    rows_detail = []

    # LOPO ì „ì²´ ìš”ì•½ìš©: PIDë³„ í‰ê·  r2 ë“±ì„ ë‚˜ì¤‘ì— ì§‘ê³„
    # (ì—¬ê¸°ì„œëŠ” ë°”ë¡œ DataFrameìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬)

    # ----------------------------------------------------
    # 3) PID ë£¨í”„ (LOPO core)
    # ----------------------------------------------------
    for idx, tgt_pid in enumerate(unique_pids):
        print("\n" + "=" * 60)
        print(f"[LOPO] {idx + 1}/{len(unique_pids)}  Target PID = {tgt_pid}")
        print("=" * 60)

        mask_test = (pid == tgt_pid)
        mask_tr   = ~mask_test

        if not np.any(mask_test):
            print(f"[WARN] PID={tgt_pid} ì— í•´ë‹¹í•˜ëŠ” ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µ.")
            continue

        # ---------- 3-1) HV mask (TRAIN ê¸°ì¤€) ----------
        if hv_mode == "none":
            keep_all = np.ones_like(y, dtype=bool)
        elif hv_mode == "x_variance":
            keep_all = hv_mask_from_train_x(X, train_mask=mask_tr, q=hv_quantile)
        elif hv_mode == "y_train":
            keep_all = hv_mask_from_train_y(
                y_all=y,
                pid_all=pid,
                scene_all=scene,
                train_mask=mask_tr,
                q=hv_quantile,
            )
        else:
            raise ValueError(f"Unknown hv_mode={hv_mode}")

        # HV ë§ˆìŠ¤í¬ + train/test ë¶„ë¦¬
        train_mask_final = mask_tr & keep_all
        test_mask_final  = mask_test & keep_all

        if not np.any(train_mask_final):
            print(f"[WARN] PID={tgt_pid} train_mask_final is empty. ìŠ¤í‚µ.")
            continue
        if not np.any(test_mask_final):
            print(f"[WARN] PID={tgt_pid} test_mask_final is empty after HV mask. ìŠ¤í‚µ.")
            continue

        X_tr_raw = X[train_mask_final]
        y_tr_raw = y[train_mask_final]
        pid_tr   = pid[train_mask_final]
        scene_tr = scene[train_mask_final]

        X_te_raw = X[test_mask_final]
        y_te_raw = y[test_mask_final]
        scene_te = scene[test_mask_final]

        print(f"[LOPO] PID={tgt_pid} | train={len(y_tr_raw)}, test={len(y_te_raw)} (after HV mask)")

        # ---------- 3-2) y centering (train ê¸°ì¤€) ----------
        center_fn, stat_y = center_from_train_split(
            y_tr_raw,
            pid_tr,
            scene_tr,
        )
        y_tr = center_fn(y_tr_raw, pid_tr, scene_tr)
        y_te = center_fn(y_te_raw, pid[test_mask_final], scene_te)

        # ---------- 3-3) X scene-wise zscore (train ê¸°ì¤€) ----------
        scene_stats, global_stats = _fit_scene_stats(X_tr_raw, scene_tr)
        X_tr = _transform_scenewise(X_tr_raw, scene_tr, scene_stats, global_stats)
        X_te = _transform_scenewise(X_te_raw, scene_te, scene_stats, global_stats)

        # ---------- 3-4) Huber loss delta ì„¤ì • (train y ê¸°ì¤€) ----------
        iqr = float(np.subtract(*np.percentile(y_tr, [75, 25])))
        delta = float(max(0.1, min(iqr, 5.0)))
        criterion = torch.nn.HuberLoss(delta=delta)

        # ---------- 3-5) train_and_evaluate_seeds í˜¸ì¶œ ----------
        # ë‚´ë¶€ì—ì„œ train/val splitì„ í•˜ë„ë¡ ì„¤ì •
        all_train_losses, all_val_losses, all_test_scores, all_train_scores, all_val_scores = (
            train_and_evaluate_seeds(
                X_trainval=X_tr,
                y_trainval=y_tr,
                pid_trainval=pid_tr,
                X_test=X_te,
                y_test=y_te,
                model_type="CNN",
                best_params=base_params,
                device=device,
                num_seeds=num_seeds,
                num_epochs=num_epochs,
                patience=patience,
                min_delta=min_delta,
                use_internal_split=True,
                external_val_data=None,
                deterministic=True,
                criterion=criterion,
                internal_split_mode="train_val",
                internal_val_ratio=internal_val_ratio,
            )
        )

        # all_test_scores: [(r2, rmse, mae), ...] êµ¬ì¡°ë¥¼ ê°€ì •
        for s_idx, (r2, rmse, mae) in enumerate(all_test_scores):
            row = {
                "target_pid": tgt_pid,
                "seed_index": s_idx,
                "seed": seed_list[s_idx] if s_idx < len(seed_list) else np.nan,
                "test_r2": float(r2) if r2 is not None else np.nan,
                "test_rmse": float(rmse) if rmse is not None else np.nan,
                "test_mae": float(mae) if mae is not None else np.nan,
                "n_train": int(len(y_tr)),
                "n_test": int(len(y_te)),
            }
            rows_detail.append(row)

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if device.type == "cuda":
            torch.cuda.empty_cache()

    # ----------------------------------------------------
    # 4) ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
    # ----------------------------------------------------
    if len(rows_detail) == 0:
        print("[LOPO] ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PID/ë§ˆìŠ¤í¬ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    df_detail = pd.DataFrame(rows_detail)
    detail_path = os.path.join(out_dir, "lopo_detail_cnn.csv")
    df_detail.to_csv(detail_path, index=False, encoding="utf-8")
    print(f"[LOPO] Saved LOPO detail metrics to: {detail_path}")

    # PIDë³„ ìš”ì•½ (mean Â± std)
    df_summary = (
        df_detail
        .groupby("target_pid")
        .agg(
            n_seeds=("seed", "count"),
            n_train=("n_train", "max"),
            n_test=("n_test", "max"),
            r2_mean=("test_r2", "mean"),
            r2_std=("test_r2", "std"),
            rmse_mean=("test_rmse", "mean"),
            rmse_std=("test_rmse", "std"),
            mae_mean=("test_mae", "mean"),
            mae_std=("test_mae", "std"),
        )
        .reset_index()
    )
    summary_path = os.path.join(out_dir, "lopo_summary_cnn.csv")
    df_summary.to_csv(summary_path, index=False, encoding="utf-8")
    print(f"[LOPO] Saved LOPO per-PID summary to: {summary_path}")

    # ì „ì²´ í‰ê·  í•œ ë²ˆ ë” ì¶œë ¥
    print("\n===== LOPO overall (í‰ê·  over PIDs) =====")
    print(
        f"RÂ² mean over PIDs:  {df_summary['r2_mean'].mean():.4f} "
        f"(SD={df_summary['r2_mean'].std():.4f})"
    )
    print(
        f"RMSE mean over PIDs: {df_summary['rmse_mean'].mean():.4f}, "
        f"MAE mean over PIDs: {df_summary['mae_mean'].mean():.4f}"
    )

    # meta ì •ë³´ë„ ê°™ì´ ì €ì¥í•´ë‘ë©´ ë‚˜ì¤‘ì— ë…¼ë¬¸ ì“¸ ë•Œ í¸í•¨
    meta = {
        "data_dir": data_dir,
        "out_dir": out_dir,
        "model_type": "CNN",
        "best_cnn_params": best_cnn_params,
        "hv_mode": hv_mode,
        "hv_quantile": hv_quantile,
        "num_seeds": num_seeds,
        "base_seed": base_seed,
        "num_epochs": num_epochs,
        "internal_val_ratio": internal_val_ratio,
        "patience": patience,
        "min_delta": min_delta,
        "device": str(device),
        "n_pids": int(len(unique_pids)),
        "N_total_windows": int(len(y)),
        "detail_csv": os.path.basename(detail_path),
        "summary_csv": os.path.basename(summary_path),
    }
    meta_path = os.path.join(out_dir, "meta_lopo_cnn.json")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f"[LOPO] Saved meta to: {meta_path}")
