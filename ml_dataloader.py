import os
import numpy as np
import pandas as pd
import neurokit2 as nk
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
# === [ADD] Robust baseline utilities =========================================
def _robust_center_scale(arr, *, use_median=True, mad_c=1.4826, eps=1e-6):
    """
    1D ë°°ì—´ì˜ ì¤‘ì‹¬/ìŠ¤ì¼€ì¼ ê³„ì‚°.
    - use_median=True  â†’ (median, mad_c * MAD)
    - use_median=False â†’ (mean, std)
    """
    a = np.asarray(arr, dtype=np.float32)
    if use_median:
        med = np.median(a)
        mad = np.median(np.abs(a - med))
        sc = float(mad_c * mad)
        if not np.isfinite(sc) or sc < eps:
            sc = 1.0
        return float(med), sc
    else:
        mu = float(np.nanmean(a))
        sd = float(np.nanstd(a))
        if not np.isfinite(sd) or sd < eps:
            sd = 1.0
        return mu, sd

def _make_baseline_fn(mode: str, *, first_seconds: int, fs: int, mad_c=1.4826):
    """
    mode: "first10s_meanstd" | "first10s_medmad" | "scene_medmad"
    ë°˜í™˜: baseline_center_scale(series: np.ndarray) -> (center, scale)
    """
    def _fn(series: np.ndarray):
        x = np.asarray(series, dtype=np.float32)
        if mode == "scene_medmad":
            return _robust_center_scale(x, use_median=True, mad_c=mad_c)
        elif mode == "first10s_medmad":
            L = max(1, int(first_seconds * fs))
            base = x[:L]
            return _robust_center_scale(base, use_median=True, mad_c=mad_c)
        else:  # "first10s_meanstd"
            L = max(1, int(first_seconds * fs))
            base = x[:L]
            return _robust_center_scale(base, use_median=False, mad_c=mad_c)
    return _fn
# ============================================================================

def _ema_causal(x: np.ndarray, alpha: float):
    """
    Causal EMA: y[t] = alpha*x[t] + (1-alpha)*y[t-1]
    alpha = 1 - exp(-Î”t/Ï„).  Î”t=1/fs, Ï„: seconds
    """
    x = np.asarray(x, dtype=np.float32)
    if len(x) == 0:
        return x
    y = np.empty_like(x)
    y[0] = x[0]
    one_minus = 1.0 - alpha
    for t in range(1, len(x)):
        y[t] = alpha * x[t] + one_minus * y[t-1]
    return y


def _rolling_slope(x: np.ndarray, k: int):
    """
    Causal rolling slope over last k samples (linear regression on indices [0..k-1]).
    ì²« (k-1) êµ¬ê°„ì€ ì²« ìœ íš¨ ê¸°ìš¸ê¸°ë¡œ ì±„ì›€.
    """
    x = np.asarray(x, dtype=np.float32)
    n = len(x)
    if k <= 1 or n == 0:
        return np.zeros_like(x, dtype=np.float32)

    # ë¯¸ë¦¬ ê³ ì •ëœ ì‹œê°„ì¶• í†µê³„(0..k-1) ì¤€ë¹„
    t = np.arange(k, dtype=np.float32)
    t_mean = t.mean()
    denom = float(np.sum((t - t_mean) ** 2)) + 1e-12  # var(t)*k

    out = np.empty_like(x, dtype=np.float32)
    first_slope = None
    for i in range(n):
        j0 = max(0, i - k + 1)
        seg = x[j0:i+1]
        if len(seg) < k:
            # ê¸¸ì´ k ë˜ê¸° ì „ì—” slope ê³„ì‚°ì„ ë’¤ë¡œ ë¯¸ë£¸
            out[i] = 0.0
            continue
        # ê¸¸ì´ ì •í™•íˆ kì¸ êµ¬ê°„ë§Œ ì‚¬ìš©
        y = seg.astype(np.float32)
        y_mean = y.mean()
        # cov(y,t) / var(t)
        num = float(np.sum((y - y_mean) * (t - t_mean)))
        slope = num / denom
        if first_slope is None:
            first_slope = slope
        out[i] = slope

    # ì• êµ¬ê°„ ì±„ìš°ê¸°
    if first_slope is None:
        first_slope = 0.0
    for i in range(min(k-1, n)):
        out[i] = first_slope
    return out


def _welch_bandpowers_fft(x: np.ndarray, fs: float, bands: list[tuple[float,float,str]]):
    """
    ê°„ë‹¨ FFT-PSD ê¸°ë°˜ ë°´ë“œíŒŒì›Œ(ì°½ ì „ì²´)ë¥¼ êµ¬í•˜ê³  dictë¡œ ë°˜í™˜.
    bands: [(f_lo, f_hi, tag), ...]
    ë˜í•œ total(0..Nyq)ê³¼ ratio(Low/All, Low/Mid) ê³„ì‚°ì„ ìœ„í•´ totalê³¼ midë„ í•¨ê»˜ ë°˜í™˜.
    """
    x = np.asarray(x, dtype=np.float32)
    n = len(x)
    if n == 0:
        return {}, 0.0

    x = x - np.mean(x)
    X = np.fft.rfft(x)                       # N/2+1
    freqs = np.fft.rfftfreq(n, d=1.0/fs)
    psd = (np.abs(X) ** 2) / (n * fs)        # ê°„ë‹¨ PSD ê·¼ì‚¬

    res = {}
    total = float(np.sum(psd))               # 0..Nyquist ì´íŒŒì›Œ
    for (flo, fhi, tag) in bands:
        m = (freqs >= flo) & (freqs < fhi)
        res[tag] = float(np.sum(psd[m]))
    return res, total

# def process_physiology_data(
#     data_path,
#     output_path="./ml_processed",
#     window_seconds=20,
#     stride_seconds=2,
#     sampling_rate=120,
#     scenename="Hallway"
    
# ):
#     os.makedirs(output_path, exist_ok=True)

#     window_size = sampling_rate * window_seconds
#     stride_size = sampling_rate * stride_seconds

#     valid_cols = {
#         "EDA": ["EDA_Tonic", "EDA_Phasic", "SCR_Amplitude", "SCR_RiseTime"],
#         "PPG": ["PPG_Rate"],
#         "RSP": ["RSP_Rate", "RSP_RVT", "RSP_Amplitude"],
#         "Pupil": ["pupilL", "pupilR", "pupil_mean"]
#     }

#     clip_dict = {
#         "EDA_Tonic": 30, "EDA_Phasic": 10, "SCR_Amplitude": 10, "SCR_RiseTime": 10,
#         "PPG_Rate": 5, "RSP_Rate": 5, "RSP_RVT": 7, "RSP_Amplitude": 10,
#         "pupilL": 10, "pupilR": 10, "pupil_mean": 10
#     }

#     participants = sorted([f.split("_")[0] for f in os.listdir(data_path) if f.endswith("_Main.pkl")])

#     baseline_dict = {}
#     anxiety_baseline_dict = {}
#     all_features = []
#     X_array = []
#     y_array = []
#     pid_array = []
#     feature_tag_list = []

#     for pid in tqdm(participants, desc="Processing"):
#         try:
#             df = pd.read_pickle(os.path.join(data_path, f"{pid}_Main.pkl"))
#             df = df[df["scene"] == scenename].dropna().reset_index(drop=True)

#             if "pupilL" in df.columns and "pupilR" in df.columns:
#                 df["pupil_mean"] = df[["pupilL", "pupilR"]].mean(axis=1)

#             base = df.iloc[:sampling_rate * 10]
#             baseline_dict[pid] = {
#                 col: (base[col].mean(), base[col].std() if base[col].std() > 1e-6 else 1.0)
#                 for mod, cols in valid_cols.items() for col in cols if col in base.columns
#             }

#             if "anxiety" in base.columns:
#                 mean = base["anxiety"].mean()
#                 std = base["anxiety"].std()
#                 anxiety_baseline_dict[pid] = (mean, std if std > 0.5 else 1.0)

#             for start in range(0, len(df) - window_size + 1, stride_size):
#                 window = df.iloc[start:start + window_size].copy()
#                 if len(window) < window_size:
#                     continue

#                 norm_window = window.copy()
#                 for mod, cols in valid_cols.items():
#                     for col in cols:
#                         if col in norm_window.columns and col in baseline_dict[pid]:
#                             mean, std = baseline_dict[pid][col]
#                             norm_window[col] = (norm_window[col] - mean) / std

#                 if "PPG_Clean" in norm_window.columns:
#                     quality = nk.ppg_quality(norm_window["PPG_Clean"].values, sampling_rate=sampling_rate)
#                     if np.nanmean(quality) < 0.5:
#                         continue

#                 try:
#                     peaks = np.where(window["PPG_Peaks"].values == 1)[0]
#                     if len(peaks) >= 4:
#                         ibi = np.diff(peaks) / sampling_rate
#                         cv = np.std(ibi) / np.mean(ibi)
#                         if cv > 0.5:
#                             raise ValueError(f"High HRV CV: {cv:.2f}")
#                         hrv = nk.hrv_time(peaks, sampling_rate=sampling_rate, show=False, method="time")
#                         hrv_features = hrv[["HRV_RMSSD", "HRV_SDNN", "HRV_pNN50"]].iloc[0].to_dict()
#                     else:
#                         hrv_features = {"HRV_RMSSD": np.nan, "HRV_SDNN": np.nan, "HRV_pNN50": np.nan}
#                 except Exception:
#                     hrv_features = {"HRV_RMSSD": np.nan, "HRV_SDNN": np.nan, "HRV_pNN50": np.nan}

#                 row = {"participant": pid, "start_idx": start}
#                 if "anxiety" in window.columns and pid in anxiety_baseline_dict:
#                     mean, std = anxiety_baseline_dict[pid]
#                     z_scored = (window["anxiety"] - mean) / std
#                     row["anxiety"] = z_scored.mean()
#                     y_array.append(z_scored.mean())

#                 feature_sequence = []
#                 feature_tags = []

#                 for mod, cols in valid_cols.items():
#                     for col in cols:
#                         if col in norm_window.columns:
#                             clipped = norm_window[col].clip(-clip_dict[col], clip_dict[col])
#                             row[f"{col}_mean"] = clipped.mean()
#                             row[f"{col}_std"] = clipped.std()
#                             row[f"{col}_max"] = clipped.max()
#                             row[f"{col}_slope"] = np.polyfit(np.arange(len(clipped)), clipped, 1)[0]
#                             feature_sequence.append(clipped.values)
#                             feature_tags.append(f"{col}")

#                 if feature_sequence:
#                     X_array.append(np.stack(feature_sequence, axis=1))  # [T, C]
#                     pid_array.append(pid)
#                     feature_tag_list.append(feature_tags)

#                 row.update(hrv_features)
#                 all_features.append(row)

#         except Exception as e:
#             print(f"[{pid}] ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
#             continue

#     df_feat = pd.DataFrame(all_features)
#     X_array = np.array(X_array)
#     y_array = np.array(y_array)
#     pid_array = np.array(pid_array)
#     feature_tag_list = feature_tag_list[0] if feature_tag_list else []

#     np.save(os.path.join(output_path, "X_array.npy"), X_array)
#     np.save(os.path.join(output_path, "y_array.npy"), y_array)
#     np.save(os.path.join(output_path, "pid_array.npy"), pid_array)
#     np.save(os.path.join(output_path, "feature_tag_list.npy"), feature_tag_list)
#     df_feat.to_csv(os.path.join(output_path, "df_feat.csv"), index=False)

#     print("âœ… ì €ì¥ ì™„ë£Œ:", output_path)
#     print(f"ğŸ“Š X shape: {X_array.shape} | y shape: {y_array.shape} | feature dim: {len(feature_tag_list)}")
def process_physiology_data(
    data_path,
    output_path="./ml_processed",
    window_seconds=20,
    stride_seconds=2,
    sampling_rate=120,
    scenename="Hallway",
    *,
    # === NEW: baseline options ===
    baseline_mode="first10s_medmad",   # "first10s_meanstd" | "first10s_medmad" | "scene_medmad"
    baseline_first_seconds=10,
    mad_c=1.4826,
    eps=1e-6
):
    """
    ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (sceneë³„):
      - pupil_mean ìƒì„±
      - baseline_modeì— ë”°ë¼ scene ë‚´ baseline(z-score) ì ìš©
      - ìœˆë„ìš°ë§ + í´ë¦¬í•‘ + feature ìš”ì•½(í‰ê· /í‘œì¤€í¸ì°¨/ìµœëŒ€/ê¸°ìš¸ê¸°)
      - HRV(time) ì¼ë¶€ ì¶”ì¶œ
      - anxietyëŠ” baseline ê¸°ì¤€ìœ¼ë¡œ z-score í›„, ìœˆë„ í‰ê· ì„ íƒ€ê¹ƒìœ¼ë¡œ ì‚¬ìš©

    baseline_mode:
      - "first10s_meanstd" : scene ì‹œì‘ 10ì´ˆì˜ mean/std
      - "first10s_medmad"  : scene ì‹œì‘ 10ì´ˆì˜ median/MAD(Ã—1.4826)
      - "scene_medmad"     : scene ì „ì²´ median/MAD(Ã—1.4826)
    """
    import os
    import numpy as np
    import pandas as pd
    import neurokit2 as nk
    from tqdm import tqdm

    os.makedirs(output_path, exist_ok=True)

    window_size = int(sampling_rate * window_seconds)
    stride_size = int(sampling_rate * stride_seconds)

    valid_cols = {
        "EDA": ["EDA_Tonic", "EDA_Phasic", "SCR_Amplitude", "SCR_RiseTime"],
        "PPG": ["PPG_Rate"],
        "RSP": ["RSP_Rate", "RSP_RVT", "RSP_Amplitude"],
        "Pupil": ["pupilL", "pupilR", "pupil_mean"]
    }

    clip_dict = {
        "EDA_Tonic": 30, "EDA_Phasic": 10, "SCR_Amplitude": 10, "SCR_RiseTime": 10,
        "PPG_Rate": 5, "RSP_Rate": 5, "RSP_RVT": 7, "RSP_Amplitude": 10,
        "pupilL": 10, "pupilR": 10, "pupil_mean": 10
    }

    # ì°¸ê°€ì ëª©ë¡
    participants = sorted([f.split("_")[0] for f in os.listdir(data_path) if f.endswith("_Main.pkl")])

    baseline_dict = {}
    anxiety_baseline_dict = {}
    all_features = []
    X_array, y_array, pid_array = [], [], []
    feature_tag_list = []

    # === baseline í•¨ìˆ˜ êµ¬ì„± (ì´ë¯¸ ìƒë‹¨ì— ì¶”ê°€í•œ helper ì‚¬ìš© ê°€ì •) ===
    baseline_fn = _make_baseline_fn(
        baseline_mode, first_seconds=baseline_first_seconds, fs=sampling_rate, mad_c=mad_c
    )

    for pid in tqdm(participants, desc="Processing"):
        try:
            df = pd.read_pickle(os.path.join(data_path, f"{pid}_Main.pkl"))
            df = df[df.get("scene") == scenename].dropna().reset_index(drop=True)
            if df.empty:
                continue

            # pupil_mean ìƒì„±
            if "pupilL" in df.columns and "pupilR" in df.columns:
                df["pupil_mean"] = df[["pupilL", "pupilR"]].mean(axis=1)

            # ===== 1) scene-level baseline ê³„ì‚° (ëª¨ë“  ì‚¬ìš© ì»¬ëŸ¼) =====
            baseline_dict[pid] = {}
            for mod, cols in valid_cols.items():
                for col in cols:
                    if col in df.columns:
                        c, s = baseline_fn(df[col].to_numpy())
                        if not np.isfinite(s) or s < eps:
                            s = 1.0
                        baseline_dict[pid][col] = (c, s)

            # anxiety baseline (scene ê¸°ì¤€)
            if "anxiety" in df.columns:
                c, s = baseline_fn(df["anxiety"].to_numpy())
                if not np.isfinite(s) or s < eps:
                    s = 1.0
                anxiety_baseline_dict[pid] = (c, s)

            # ===== 2) ìœˆë„ìš° ë£¨í”„ =====
            for start in range(0, len(df) - window_size + 1, stride_size):
                window = df.iloc[start:start + window_size].copy()
                if len(window) < window_size:
                    continue

                # (ì˜µì…˜) í’ˆì§ˆ ì²´í¬ (PPG_Clean ìˆì„ ë•Œë§Œ)
                if "PPG_Clean" in window.columns:
                    try:
                        quality = nk.ppg_quality(window["PPG_Clean"].to_numpy(), sampling_rate=sampling_rate)
                        if float(np.nanmean(quality)) < 0.5:
                            continue
                    except Exception:
                        pass  # í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨ ì‹œ í†µê³¼(ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€)

                # ===== 2-1) scene-baseline z-score ì ìš© =====
                norm_window = window.copy()
                for mod, cols in valid_cols.items():
                    for col in cols:
                        if col in norm_window.columns and col in baseline_dict.get(pid, {}):
                            mean_c, std_c = baseline_dict[pid][col]
                            std_c = std_c if std_c > eps else 1.0
                            norm_window[col] = (norm_window[col] - mean_c) / std_c

                # ===== 2-2) HRV(time) ì¼ë¶€ ì¶”ì¶œ (PPG_Peaks ì‚¬ìš©) =====
                try:
                    hrv_features = {"HRV_RMSSD": np.nan, "HRV_SDNN": np.nan, "HRV_pNN50": np.nan}
                    if "PPG_Peaks" in window.columns:
                        peaks = np.where(window["PPG_Peaks"].to_numpy() == 1)[0]
                        if len(peaks) >= 4:
                            ibi = np.diff(peaks) / float(sampling_rate)
                            if (np.mean(ibi) > eps) and (np.std(ibi) / (np.mean(ibi) + eps) <= 0.5):
                                # neurokit2 hrv_time ì…ë ¥ì€ R-peak ì¸ë±ìŠ¤ë¥¼ ê¸°ëŒ€í•˜ì§€ë§Œ,
                                # ì—¬ê¸°ì„œëŠ” PPG peak indexë¡œ ê·¼ì‚¬ ì‚¬ìš©
                                hrv_df = nk.hrv_time(
                                    peaks, sampling_rate=sampling_rate, show=False, method="time"
                                )
                                if not hrv_df.empty:
                                    hrv_features = {
                                        "HRV_RMSSD": float(hrv_df["HRV_RMSSD"].iloc[0]) if "HRV_RMSSD" in hrv_df else np.nan,
                                        "HRV_SDNN":  float(hrv_df["HRV_SDNN"].iloc[0])  if "HRV_SDNN"  in hrv_df else np.nan,
                                        "HRV_pNN50": float(hrv_df["HRV_pNN50"].iloc[0]) if "HRV_pNN50" in hrv_df else np.nan
                                    }
                except Exception:
                    hrv_features = {"HRV_RMSSD": np.nan, "HRV_SDNN": np.nan, "HRV_pNN50": np.nan}

                # ===== 2-3) íƒ€ê¹ƒ(anxiety) ìœˆë„ í‰ê·  =====
                row = {"participant": pid, "start_idx": start}
                if "anxiety" in window.columns and pid in anxiety_baseline_dict:
                    mean_a, std_a = anxiety_baseline_dict[pid]
                    std_a = std_a if std_a > eps else 1.0
                    z_anx = (window["anxiety"] - mean_a) / std_a
                    row["anxiety"] = float(np.nanmean(z_anx))
                    y_array.append(row["anxiety"])

                # ===== 2-4) í”¼ì²˜ ìš”ì•½ + ì‹œí€€ìŠ¤ ìŒ“ê¸° =====
                feature_sequence = []
                feature_tags = []

                t_idx = np.arange(len(norm_window), dtype=np.float32)
                for mod, cols in valid_cols.items():
                    for col in cols:
                        if col in norm_window.columns:
                            # í´ë¦¬í•‘ í›„ í†µê³„/ê¸°ìš¸ê¸°
                            clipped = norm_window[col].clip(-clip_dict[col], clip_dict[col]).to_numpy(dtype=np.float32)
                            row[f"{col}_mean"]  = float(np.nanmean(clipped))
                            row[f"{col}_std"]   = float(np.nanstd(clipped))
                            row[f"{col}_max"]   = float(np.nanmax(clipped))
                            # slope (1ì°¨ ì„ í˜•íšŒê·€ ê³„ìˆ˜)
                            try:
                                # polyfitì€ NaNì´ ìˆìœ¼ë©´ ì‹¤íŒ¨ â†’ NaN ì²˜ë¦¬
                                if np.isnan(clipped).any():
                                    slope = np.nan
                                else:
                                    slope = np.polyfit(t_idx, clipped, 1)[0]
                            except Exception:
                                slope = np.nan
                            row[f"{col}_slope"] = float(slope) if np.isfinite(slope) else np.nan

                            # ì›ì‹œ ì‹œí€€ìŠ¤ [T] â†’ ë‚˜ì¤‘ ìŠ¤íƒ ì‹œ [T,C]
                            feature_sequence.append(clipped)
                            feature_tags.append(f"{col}")

                if feature_sequence:
                    X_array.append(np.stack(feature_sequence, axis=1))  # [T, C]
                    pid_array.append(pid)
                    feature_tag_list = feature_tags  # ì°½ë§ˆë‹¤ ë™ì¼ êµ¬ì„± ê°€ì • â†’ ë§ˆì§€ë§‰ ê°’ ì‚¬ìš©

                # HRV ë³‘í•©
                row.update(hrv_features)
                all_features.append(row)

        except Exception as e:
            print(f"[{pid}] ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue

    # ===== 3) ì•„ì›ƒí’‹ ì €ì¥ =====
    df_feat = pd.DataFrame(all_features)
    X_array = np.asarray(X_array, dtype=np.float32) if len(X_array) else np.empty((0, window_size, 0), dtype=np.float32)
    y_array = np.asarray(y_array, dtype=np.float32) if len(y_array) else np.empty((0,), dtype=np.float32)
    pid_array = np.asarray(pid_array)

    np.save(os.path.join(output_path, "X_array.npy"), X_array)
    np.save(os.path.join(output_path, "y_array.npy"), y_array)
    np.save(os.path.join(output_path, "pid_array.npy"), pid_array)
    np.save(os.path.join(output_path, "feature_tag_list.npy"), np.array(feature_tag_list, dtype=object))
    df_feat.to_csv(os.path.join(output_path, "df_feat.csv"), index=False)

    print("âœ… ì €ì¥ ì™„ë£Œ:", output_path)
    print(f"ğŸ“Š X shape: {X_array.shape} | y shape: {y_array.shape} | feature dim: {len(feature_tag_list)}")


# === [PATCH 1/3] Low-frequency utils (hop ê¸°ë°˜, causal) ======================
import numpy as np

def _lf_alpha_from_tau(hop_seconds: float, tau_seconds: float) -> float:
    """
    EMA alpha = 1 - exp(-hop/tau), hop/tau ëª¨ë‘ 'ì´ˆ' ë‹¨ìœ„.
    """
    hop = float(max(hop_seconds, 1e-9))
    tau = float(max(tau_seconds, 1e-9))
    return 1.0 - np.exp(-hop / tau)

def _ema_causal_hop(x: np.ndarray, hop_seconds: float, tau_seconds: float) -> np.ndarray:
    """
    Causal EMA on window sequence x (shape: (T,)).
    EMA[t] = alpha*x[t] + (1-alpha)*EMA[t-1], alpha = 1 - exp(-hop/tau).
    """
    x = np.asarray(x, dtype=np.float32)
    if x.size == 0:
        return x
    alpha = _lf_alpha_from_tau(hop_seconds, tau_seconds)
    y = np.empty_like(x, dtype=np.float32)
    y[0] = x[0]
    one_minus = 1.0 - alpha
    for t in range(1, x.shape[0]):
        y[t] = alpha * x[t] + one_minus * y[t-1]
    return y

def _rolling_slope_causal_hop(x: np.ndarray, hop_seconds: float, window_seconds: float) -> np.ndarray:
    """
    Causal rolling slope over last k samples, with k = round(window_seconds / hop_seconds).
    ë¦¬í„´ ë‹¨ìœ„: 'ì´ˆë‹¹ ë³€í™”ëŸ‰' (per second).
    """
    x = np.asarray(x, dtype=np.float32)
    T = x.shape[0]
    if T == 0:
        return x
    hop = float(max(hop_seconds, 1e-9))
    k = int(max(round(float(window_seconds) / hop), 2))  # ìµœì†Œ 2
    out = np.zeros(T, dtype=np.float32)

    # ê³ ì • ì‹œê°„ì¶• í†µê³„(0..k-1)
    t = np.arange(k, dtype=np.float32)
    t_mean = t.mean()
    denom = float(np.sum((t - t_mean) ** 2)) + 1e-12  # var(t)*k

    first_slope = None
    for i in range(T):
        j0 = i - k + 1
        if j0 < 0:
            continue  # ì• êµ¬ê°„ì€ ë‚˜ì¤‘ì— ì±„ì›€
        y = x[j0:i+1].astype(np.float32)  # ê¸¸ì´ k
        y_mean = y.mean()
        num = float(np.sum((y - y_mean) * (t - t_mean)))
        slope_per_step = num / denom
        slope_per_sec = slope_per_step / hop
        if first_slope is None:
            first_slope = slope_per_sec
        out[i] = slope_per_sec

    if first_slope is None:
        first_slope = 0.0
    # ì´ˆê¸° ë¯¸ì¶©ì¡± êµ¬ê°„ ì±„ìš°ê¸°
    for i in range(min(k-1, T)):
        out[i] = first_slope
    return out
# =============================================================================

import os
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict

# ---------------------------
# Helper: interpolation-based downsampling (column-wise)
# ---------------------------
def interpolate_downsample(df: pd.DataFrame, target_hz: int, original_hz: int = 120, time_col: str = None):
    """
    ì„ í˜•ë³´ê°„ ê¸°ë°˜ ë‹¤ìš´ìƒ˜í”Œë§. ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì²˜ë¦¬.
    time_colì´ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì»¬ëŸ¼(ë°€ë¦¬ì´ˆ) ê¸°ì¤€ìœ¼ë¡œ ë¦¬ìƒ˜í”Œ, ì—†ìœ¼ë©´ ê°€ìƒ ì‹œê°„ì¶• ì‚¬ìš©.
    target_hz >= original_hz ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    if target_hz >= original_hz:
        return df.copy()

    num_rows = len(df)
    if num_rows == 0:
        return df.copy()

    # ì›ë˜/íƒ€ê²Ÿ ì‹œê°„ì¶•
    if time_col is None:
        t_orig = np.arange(num_rows) / original_hz
    else:
        t_orig = (df[time_col].to_numpy() / 1000.0)

    total_time = t_orig[-1] - t_orig[0]
    n_target = int(np.floor(total_time * target_hz)) + 1
    if n_target < 2:
        n_target = max(2, int(num_rows * target_hz / original_hz))

    t_new = np.linspace(t_orig[0], t_orig[-1], n_target)

    # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ë³´ê°„
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    out = {}
    for col in numeric_cols:
        x = df[col].to_numpy()
        # NaN ì„ì‹œ ì±„ìš°ê¸°(ì•/ë’¤ í™•ì¥)
        if np.isnan(x).any():
            s = pd.Series(x).ffill().bfill().to_numpy()
        else:
            s = x
        out[col] = np.interp(t_new, t_orig, s)

    # ìˆ«ì ì•„ë‹Œ ì»¬ëŸ¼ì€ ìµœê·¼ì ‘ ì¸ë±ìŠ¤ë¡œ ì„œë¸Œìƒ˜í”Œ
    non_numeric_cols = [c for c in df.columns if c not in numeric_cols]
    if non_numeric_cols:
        idx_new = np.searchsorted(t_orig, t_new, side="left")
        idx_new = np.clip(idx_new, 0, num_rows - 1)
        for col in non_numeric_cols:
            out[col] = df[col].iloc[idx_new].to_numpy()

    return pd.DataFrame(out)
# ---------------------------
# Main: extract raw windows (scene ê³ ì • ì»¬ëŸ¼ëª… ì‚¬ìš©)
#  - ì¶”ê°€: feature expansion(ì˜µì…˜), target smoothing(ì˜µì…˜)
# ---------------------------
import os, json
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict

# ---- ê°„ë‹¨ ë¡¤ë§/ìŠ¤í™íŠ¸ëŸ¼ ìœ í‹¸ ----
def _rolling_mean(x, k):
    if k <= 1: return x.copy()
    # padding='reflect'ë¡œ ê°€ì¥ìë¦¬ ì™œê³¡ ìµœì†Œí™”
    pad = k // 2
    xpad = np.pad(x, (pad, k - 1 - pad), mode='reflect')
    ker = np.ones(k, dtype=np.float32) / k
    return np.convolve(xpad, ker, mode='valid').astype(np.float32)

def _rolling_std(x, k):
    if k <= 1: return np.zeros_like(x, dtype=np.float32)
    m = _rolling_mean(x, k)
    # (x-m)^2ì˜ í‰ê· ì˜ ë£¨íŠ¸
    pad = k // 2
    xpad = np.pad(x, (pad, k - 1 - pad), mode='reflect')
    ker = np.ones(k, dtype=np.float32) / k
    v = np.convolve((xpad - np.mean(xpad))**2, ker, mode='valid')
    # ê·¼ì‚¬: êµ­ì†Œë¶„ì‚° ëŒ€ì‹  ì „ì—­í‰ê·  ë³´ì • í”¼í•˜ê¸° ìœ„í•´ m ì´ìš©
    return np.sqrt(np.maximum(1e-12, _rolling_mean((x - m)**2, k))).astype(np.float32)

def _diff(x, order=1):
    if order == 1:
        d = np.diff(x, n=1, prepend=x[0])
    elif order == 2:
        d = np.diff(x, n=2, prepend=[x[0], x[1] if len(x) > 1 else x[0]])
    else:
        raise ValueError("order must be 1 or 2")
    return d.astype(np.float32)

def _slope_whole_window(x):
    # ì°½ ì „ì²´ì— ëŒ€í•´ ì„ í˜•íšŒê·€ ê¸°ìš¸ê¸° (ìƒìˆ˜ ì±„ë„ë¡œ ë°˜í™˜)
    n = len(x)
    t = np.arange(n, dtype=np.float32)
    t -= t.mean()
    denom = np.sum(t*t) + 1e-12
    slope = np.sum((x - x.mean()) * t) / denom
    return slope

def _iqr_whole_window(x):
    q75, q25 = np.percentile(x, [75, 25])
    return float(q75 - q25)

def _band_energy_fft(x, fs, f_lo, f_hi):
    # ì°½ ë‚´ FFT ê¸°ë°˜ ëŒ€ì—­ ì—ë„ˆì§€ (ìƒëŒ€ì  í•©)
    x = x.astype(np.float32)
    n = len(x)
    x = x - np.mean(x)
    X = np.fft.rfft(x)  # N/2+1
    freqs = np.fft.rfftfreq(n, d=1.0/fs)
    psd = (np.abs(X) ** 2) / (n * fs)  # ê°„ë‹¨ PSD ê·¼ì‚¬
    m = (freqs >= f_lo) & (freqs < f_hi)
    return float(np.sum(psd[m]))
def extract_raw_physio_windows(
    data_path: str,
    output_path: str = "./ml_processed_raw",
    window_seconds: int = 5,     # ê¶Œì¥: 5ì´ˆ
    stride_seconds: int = 5,     # ê¶Œì¥: 5ì´ˆ (ê²¹ì¹¨ ì—†ìŒ)
    sampling_rate: int = 120,
    scenes="Outside",            # None=ì „ì²´, str ë˜ëŠ” list[str]
    original_hz: int = 120,      # ì›ë³¸ ì €ì¥ ì£¼íŒŒìˆ˜(ê¸°ë³¸ 120Hzë¡œ ê°€ì •)
    save_meta: bool = True,
    # ---- íƒ€ê¹ƒ ìŠ¤ë¬´ë”© ì˜µì…˜ ----
    enable_target_smoothing: bool = False,
    target_smoothing_method: str = "ema",  # "ema" | "median"
    target_smoothing_steps: int = 3,
    smooth_before_zscore: bool = True,
    # ---- íŒŒìƒ í”¼ì²˜ ì˜µì…˜ ----
    enable_feature_expansion: bool = False,
    fe_diff_orders=(1, 2),
    fe_ma_seconds=(2,),
    fe_std_seconds=(5,),
    fe_enable_slope=True,
    fe_enable_iqr=True,
    fe_enable_band_energy=True,
    # ---- ì €ì—­ ë²ˆë“¤ ì˜µì…˜ ----
    fe_enable_lowfreq_bundle: bool = False,
    fe_lowfreq_hop_seconds: float = None,
    fe_lowfreq_targets: tuple = ("EDA_Tonic","EDA_Phasic","PPG_Rate","RSP_Rate","pupilL"),
    fe_lowfreq_spec: dict = None,
    lf_ema_seconds: tuple = (10,),
    lf_slope_seconds: tuple = (10,),
    lf_bands: tuple = ((0.00, 0.02, "LF"), (0.02, 0.08, "MF"), (0.08, 0.20, "HF")),
    # ==== âœ… ì‹ ê·œ: baseline ëª¨ë“œ ì˜µì…˜ ====
    baseline_mode_signals: str = None,     # None | "first10s_meanstd" | "first10s_medmad" | "scene_medmad"
    baseline_mode_target: str  = None,     # None | ë™ì¼ ì„ íƒì§€
    baseline_first_seconds: int = 10,
    mad_c: float = 1.4826,
    eps: float = 1e-6
):
    """
    ì¶œë ¥: X_array [N, C, T], y_array [N], pid_array [N], scene_array [N], windex_array [N]
    feature_tag_list.npy, (ì„ íƒ) meta.json

    ë³€ê²½ì (ì¤‘ìš”):
      â€¢ baseline_mode_signals / baseline_mode_target ë¡œ scene-level baseline 1ì°¨ ì •ê·œí™” ì§€ì›
      â€¢ ê¸°ì¡´ ì°½ ë‚´ë¶€ z-scoreëŠ” ìœ ì§€(2ë‹¨ ì •ê·œí™”)
    """
    import os, json
    import numpy as np
    import pandas as pd
    from collections import defaultdict
    from tqdm import tqdm

    # ==== ë‚´ë¶€ ìœ í‹¸(ì´ë¯¸ íŒŒì¼ ìƒë‹¨ì— ìˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥) =========================
    def _robust_center_scale(arr, *, use_median=True, mad_c=1.4826, eps=1e-6):
        a = np.asarray(arr, dtype=np.float32)
        if use_median:
            med = np.median(a)
            mad = np.median(np.abs(a - med))
            sc = float(mad_c * mad)
            if not np.isfinite(sc) or sc < eps:
                sc = 1.0
            return float(med), sc
        else:
            mu = float(np.nanmean(a))
            sd = float(np.nanstd(a))
            if not np.isfinite(sd) or sd < eps:
                sd = 1.0
            return mu, sd

    def _make_baseline_fn(mode: str, *, first_seconds: int, fs: int, mad_c=1.4826, eps=1e-6):
        def _fn(x: np.ndarray):
            x = np.asarray(x, dtype=np.float32)
            if mode == "scene_medmad":
                return _robust_center_scale(x, use_median=True, mad_c=mad_c, eps=eps)
            elif mode == "first10s_medmad":
                L = max(1, int(first_seconds * fs))
                base = x[:L]
                return _robust_center_scale(base, use_median=True, mad_c=mad_c, eps=eps)
            elif mode == "first10s_meanstd":
                L = max(1, int(first_seconds * fs))
                base = x[:L]
                return _robust_center_scale(base, use_median=False, mad_c=mad_c, eps=eps)
            else:
                # None or unknown â†’ no-op: (0,1)
                return 0.0, 1.0
        return _fn
    # ========================================================================

    os.makedirs(output_path, exist_ok=True)

    window_size = int(window_seconds * sampling_rate)
    stride_size = int(stride_seconds * sampling_rate)

    # ì‚¬ìš© ì‹ í˜¸ ì»¬ëŸ¼
    signal_dict = {
        "EDA":   ["EDA_Tonic", "EDA_Phasic", "SCR_Amplitude", "SCR_RiseTime"],
        "PPG":   ["PPG_Rate"],
        "RSP":   ["RSP_Rate", "RSP_RVT", "RSP_Amplitude"],
        "Pupil": ["pupilL", "pupilR", "pupil_mean"],
    }
    base_cols = sum(signal_dict.values(), [])
    # âœ… ì¶”ê°€ physiology ì±„ë„ (ìˆìœ¼ë©´ ì“°ê³ , ì—†ì–´ë„ ìŠ¤í‚µ)
    extra_physio_cols = [
        "EDA_Clean",
        "PPG_Clean",
        "RSP_Clean",
        "RSP_Phase",
    ]
    band_map = {
        "EDA_Tonic": [(0.0, 0.4, "EDA_0_0.4")],
        "RSP_Rate":  [(0.2, 0.5, "RSP_0.2_0.5")],
        "RSP_RVT":   [(0.2, 0.5, "RSP_0.2_0.5")],
        "RSP_Amplitude": [(0.2, 0.5, "RSP_0.2_0.5")],
        "PPG_Rate":  [(0.04, 0.15, "HRV_LF_approx"),
                      (0.15, 0.40, "HRV_HF_approx")],
    }
    # âœ… physiology cross-modality ì¡°í•© (ì°½ ë‚´ë¶€ì—ì„œ ì‚¬ìš©)
    combo_pairs = [
        # ì‹¬ë°• Ã— EDA (ì „ë°˜ì  arousal proxy)
        ("EDA_Tonic",  "PPG_Rate", "EDA_Tonic_x_PPG_Rate"),
        ("EDA_Phasic", "PPG_Rate", "EDA_Phasic_x_PPG_Rate"),
        # cardio-respiratory coupling ë¹„ìŠ·í•œ proxy
        ("RSP_Rate",   "PPG_Rate", "RSP_Rate_x_PPG_Rate"),
    ]
    ratio_pairs = [
        # ì‹¬ë°• ìˆ˜ì¤€ì„ ë‚˜ëˆˆ EDA / RSP
        ("EDA_Tonic", "PPG_Rate", "EDA_Tonic_over_PPG_Rate"),
        ("RSP_Rate",  "PPG_Rate", "RSP_Rate_over_PPG_Rate"),
    ]
    eps_div = 1e-3

    # ì°¸ê°€ì
    participants = sorted([f.split("_")[0] for f in os.listdir(data_path) if f.endswith("_Main.pkl")])

    # ê²°ê³¼ ì»¨í…Œì´ë„ˆ
    X_list, y_list, pid_list = [], [], []
    scene_list, windex_list = [], []

    # scenes ì¸ì ì •ê·œí™”
    if scenes is None:
        scenes_set = None
    elif isinstance(scenes, str):
        scenes_set = {scenes}
    else:
        scenes_set = set(scenes)

    # ë¡¤ë§ ì»¤ë„ í¬ê¸°
    ma_ks = [max(1, int(round(s * sampling_rate))) for s in fe_ma_seconds]
    std_ks = [max(1, int(round(s * sampling_rate))) for s in fe_std_seconds]

    # baseline í•¨ìˆ˜ ì¤€ë¹„(ì‹ í˜¸/íƒ€ê¹ƒ ê°ê°)
    bl_sig_fn = _make_baseline_fn(
        baseline_mode_signals, first_seconds=baseline_first_seconds, fs=sampling_rate, mad_c=mad_c, eps=eps
    )
    bl_tgt_fn = _make_baseline_fn(
        baseline_mode_target, first_seconds=baseline_first_seconds, fs=sampling_rate, mad_c=mad_c, eps=eps
    )

    for pid in tqdm(participants, desc="Extracting Raw Signals"):
        try:
            df = pd.read_pickle(os.path.join(data_path, f"{pid}_Main.pkl"))
            if 'scene' not in df.columns:
                df['scene'] = 'unknown'

            # scene í•„í„°ë§
            if scenes_set is None:
                df_scene_all = df.copy()
            else:
                df_scene_all = df[df['scene'].isin(scenes_set)].copy()
            if df_scene_all.empty or "anxiety" not in df_scene_all.columns:
                continue

            # pupil_mean ìƒì„±
            if "pupil_mean" not in df_scene_all.columns and {"pupilL", "pupilR"}.issubset(df_scene_all.columns):
                df_scene_all["pupil_mean"] = df_scene_all[["pupilL", "pupilR"]].mean(axis=1)

            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€ + ê²°ì¸¡ ì œê±°
            keep_cols = ["scene", "anxiety"] + [c for c in base_cols if c in df_scene_all.columns]
            df_scene_all = df_scene_all[keep_cols].dropna().reset_index(drop=True)
            if len(df_scene_all) < window_size:
                continue

            # ë‹¤ìš´ìƒ˜í”Œ(í•„ìš” ì‹œ)
            if sampling_rate < original_hz:
                df_scene_all = interpolate_downsample(
                    df_scene_all, target_hz=sampling_rate, original_hz=original_hz
                )

            # ==== íƒ€ê¹ƒ(y) ì²˜ë¦¬ ====
            anxiety_raw = df_scene_all["anxiety"].to_numpy(dtype=np.float32)

            # (ì˜µì…˜) ìŠ¤ë¬´ë”©
            if enable_target_smoothing:
                k = max(1, int(target_smoothing_steps))
                if target_smoothing_method.lower() == "median":
                    if k % 2 == 0: k += 1
                    pad = k // 2
                    xp = np.pad(anxiety_raw, (pad, pad), mode="reflect")
                    sm = np.array([np.median(xp[i:i+k]) for i in range(len(xp)-k+1)], dtype=np.float32)
                else:
                    alpha = 2.0 / (k + 1.0)
                    sm = np.empty_like(anxiety_raw)
                    acc = anxiety_raw[0]
                    for i, v in enumerate(anxiety_raw):
                        acc = alpha * v + (1 - alpha) * acc
                        sm[i] = acc
                anxiety_for_norm = sm if smooth_before_zscore else anxiety_raw
            else:
                anxiety_for_norm = anxiety_raw

            # (ì„ íƒ) scene-level baseline 1ì°¨ ì •ê·œí™”
            c_t, s_t = bl_tgt_fn(anxiety_for_norm)
            if s_t < eps: s_t = 1.0
            anxiety_bl = (anxiety_for_norm - c_t) / s_t

            # (ê¸°ì¡´) scene ì „ì²´ z-score (ì›í•˜ë©´ ìƒëµ ê°€ëŠ¥)
            a_mean, a_std = float(np.nanmean(anxiety_bl)), float(np.nanstd(anxiety_bl))
            if not np.isfinite(a_std) or a_std < eps: a_std = 1.0
            anxiety_z = (anxiety_bl - a_mean) / a_std

            # ëª¨ë“  "í•„ìˆ˜" physiology ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            present_main = [c for c in base_cols if c in df_scene_all.columns]
            if len(present_main) != len(base_cols):
                # í•„ìˆ˜ ì±„ë„ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ ì´ ì°¸ê°€ì/sceneì€ ìŠ¤í‚µ
                continue

            # ì¶”ê°€ physiology ì±„ë„ì€ ìˆìœ¼ë©´ ê°™ì´ ì‚¬ìš©
            extra_present = [c for c in extra_physio_cols if c in df_scene_all.columns]

            # ìµœì¢… ì‚¬ìš©í•  ì±„ë„ ëª©ë¡ = í•„ìˆ˜ + ì¶”ê°€
            present_cols = present_main + extra_present

            # ì›ë³¸ ì‹œê³„ì—´ ìºì‹œ
            series_map = {c: df_scene_all[c].to_numpy(dtype=np.float32) for c in present_cols}


            # (ì„ íƒ) ì‹ í˜¸ scene-level baseline 1ì°¨ ì •ê·œí™”
            if baseline_mode_signals is not None:
                for col in present_cols:
                    c_s, s_s = bl_sig_fn(series_map[col])
                    if s_s < eps: s_s = 1.0
                    series_map[col] = (series_map[col] - c_s) / s_s

            # ìœˆë„ ë£¨í”„
            n = len(df_scene_all)
            scene_series = df_scene_all['scene'].to_numpy()
            widx_counter = defaultdict(int)

            for start in range(0, n - window_size + 1, stride_size):
                end = start + window_size

                # scene ê²½ê³„ ì•ˆì „
                window_scenes = scene_series[start:end]
                if np.any(window_scenes != window_scenes[0]):
                    continue
                sc_name = str(window_scenes[0])

                channel_data, channel_tags = [], []
                t_idx = np.arange(window_size, dtype=np.float32)

                # ì±„ë„ í™•ì¥ + ì°½ ë‚´ë¶€ z-score
                for col in present_cols:
                    seg = series_map[col][start:end]  # scene-baseline ë°˜ì˜ëœ ì›ë³¸ ì°½

                    candidates = [(seg, col)]
                    if enable_feature_expansion:
                        # 1) ì°¨ë¶„
                        for od in fe_diff_orders:
                            d = _diff(seg, order=od)
                            candidates.append((d, f"{col}_diff{od}"))
                        # 2) ì´ë™í†µê³„
                        for klen in ma_ks:
                            ma = _rolling_mean(seg, klen)
                            candidates.append((ma, f"{col}_ma{klen}"))
                        for klen in std_ks:
                            rs = _rolling_std(seg, klen)
                            candidates.append((rs, f"{col}_std{klen}"))
                        # 3) slope/IQR/ëŒ€ì—­ì—ë„ˆì§€(ìƒìˆ˜ì±„ë„)
                        if fe_enable_slope:
                            s = _slope_whole_window(seg)
                            candidates.append((np.full_like(seg, s), f"{col}_slope"))
                        if fe_enable_iqr:
                            q = _iqr_whole_window(seg)
                            candidates.append((np.full_like(seg, q), f"{col}_iqr"))
                        if fe_enable_band_energy and col in band_map:
                            for (flo, fhi, tag) in band_map[col]:
                                be = _band_energy_fft(seg, sampling_rate, flo, fhi)
                                candidates.append((np.full_like(seg, be), f"{col}_{tag}"))
                        # 4) ì €ì—­ ë²ˆë“¤(ì˜µì…˜)
                        if fe_enable_lowfreq_bundle and (col in fe_lowfreq_targets):
                            _hop_sec = float(fe_lowfreq_hop_seconds) if fe_lowfreq_hop_seconds is not None else float(stride_seconds)
                            _spec = dict(ema_taus=list(lf_ema_seconds), slope_secs=list(lf_slope_seconds), use_bandpower=False)
                            if isinstance(fe_lowfreq_spec, dict): _spec.update(fe_lowfreq_spec)
                            for tau in _spec.get("ema_taus", []) or []:
                                ema = _ema_causal_hop(seg, hop_seconds=_hop_sec, tau_seconds=float(tau))
                                candidates.append((ema.astype(np.float32), f"{col}__LF_EMA_{int(round(tau))}s"))
                            for wsec in _spec.get("slope_secs", []) or []:
                                rs = _rolling_slope_causal_hop(seg, hop_seconds=_hop_sec, window_seconds=float(wsec))
                                candidates.append((rs.astype(np.float32), f"{col}__LF_RSLOPE_{int(round(wsec))}s"))

                    # ---- ê° í›„ë³´ë¥¼ ì°½ ë‚´ë¶€ z-score í›„ ì±„ë„ì— ì¶”ê°€ ----
                    for arr, tag in candidates:
                        if arr is None:
                            continue
                        m = float(np.nanmean(arr))
                        s = float(np.nanstd(arr))
                        if not np.isfinite(s) or s < eps:
                            s = 1.0
                        z = (arr - m) / s
                        channel_data.append(z.astype(np.float32))
                        channel_tags.append(tag)

                # âœ… (ì¶”ê°€) physiology cross-modality ì¡°í•© ì±„ë„
                if enable_feature_expansion:
                    # 1) ê³±ì…ˆ ê¸°ë°˜ ì¡°í•©
                    for a, b, name in combo_pairs:
                        if a in series_map and b in series_map:
                            seg_a = series_map[a][start:end]
                            seg_b = series_map[b][start:end]
                            arr = seg_a * seg_b
                            m = float(np.nanmean(arr))
                            s = float(np.nanstd(arr))
                            if not np.isfinite(s) or s < eps:
                                s = 1.0
                            z = (arr - m) / s
                            channel_data.append(z.astype(np.float32))
                            channel_tags.append(name)

                    # 2) ratio ê¸°ë°˜ ì¡°í•©
                    for a, b, name in ratio_pairs:
                        if a in series_map and b in series_map:
                            seg_a = series_map[a][start:end]
                            seg_b = series_map[b][start:end]
                            arr = seg_a / (np.abs(seg_b) + eps_div)
                            m = float(np.nanmean(arr))
                            s = float(np.nanstd(arr))
                            if not np.isfinite(s) or s < eps:
                                s = 1.0
                            z = (arr - m) / s
                            channel_data.append(z.astype(np.float32))
                            channel_tags.append(name)

                X = np.stack(channel_data, axis=0)   # [C, T]
                y = float(np.nanmean(anxiety_z[start:end]))

                # ë©”íƒ€
                widx = widx_counter[(pid, sc_name)]
                widx_counter[(pid, sc_name)] += 1

                X_list.append(X)
                y_list.append(y)
                pid_list.append(pid)
                scene_list.append(sc_name)
                windex_list.append(widx)

        except Exception as e:
            print(f"[{pid}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    if len(X_list) == 0:
        print("âš ï¸ ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. scene í•„í„°/ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # ë°°ì—´í™” & ì €ì¥
    X_array = np.asarray(X_list, dtype=np.float32)         # [N, C, T]
    y_array = np.asarray(y_list, dtype=np.float32)         # [N]
    pid_array = np.asarray(pid_list)                       # [N]
    scene_array = np.asarray(scene_list)                   # [N]
    windex_array = np.asarray(windex_list, dtype=np.int32) # [N]
    feature_tags = np.array(channel_tags, dtype="U128")    # ë§ˆì§€ë§‰ ì°½ì˜ íƒœê·¸(ë™ì¼ êµ¬ì„± ê°€ì •)

    np.save(os.path.join(output_path, "X_array.npy"), X_array)
    np.save(os.path.join(output_path, "y_array.npy"), y_array)
    np.save(os.path.join(output_path, "pid_array.npy"), pid_array)
    np.save(os.path.join(output_path, "scene_array.npy"), scene_array)
    np.save(os.path.join(output_path, "windex_array.npy"), windex_array)
    np.save(os.path.join(output_path, "feature_tag_list.npy"), feature_tags)

    if save_meta:
        meta = {
            "sampling_rate": sampling_rate,
            "original_hz": original_hz,
            "window_seconds": window_seconds,
            "stride_seconds": stride_seconds,
            "scenes": list(scenes_set) if scenes_set is not None else "ALL",
            "n_windows": int(len(X_array)),
            "n_participants": int(len(np.unique(pid_array))),
            "enable_target_smoothing": enable_target_smoothing,
            "target_smoothing_method": target_smoothing_method,
            "target_smoothing_steps": int(target_smoothing_steps),
            "smooth_before_zscore": smooth_before_zscore,
            "enable_feature_expansion": enable_feature_expansion,
            "fe_diff_orders": list(fe_diff_orders),
            "fe_ma_seconds": list(fe_ma_seconds),
            "fe_std_seconds": list(fe_std_seconds),
            "fe_enable_slope": fe_enable_slope,
            "fe_enable_iqr": fe_enable_iqr,
            "fe_enable_band_energy": fe_enable_band_energy,
            "feature_cols_base": base_cols,
            "feature_cols_final": feature_tags.tolist(),
            # ì‹ ê·œ baseline ì„¤ì • ê¸°ë¡
            "baseline_mode_signals": baseline_mode_signals,
            "baseline_mode_target": baseline_mode_target,
            "baseline_first_seconds": baseline_first_seconds,
            "mad_c": mad_c
        }
        with open(os.path.join(output_path, "meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

    print("âœ… ì €ì¥ ì™„ë£Œ:", output_path)
    print(f"ğŸ“Š X shape: {X_array.shape} | y shape: {y_array.shape} | #PIDs: {len(np.unique(pid_array))}")
    print(f"ğŸ§© Channels: {X_array.shape[1]} | (ì˜ˆ: {feature_tags[:min(10,len(feature_tags))]})")
    print("ğŸ“ saved: scene_array.npy, windex_array.npy, feature_tag_list.npy" + (", meta.json" if save_meta else ""))

# def extract_raw_physio_windows(
#     data_path: str,
#     output_path: str = "./ml_processed_raw",
#     window_seconds: int = 5,     # ê¶Œì¥: 5ì´ˆ
#     stride_seconds: int = 5,     # ê¶Œì¥: 5ì´ˆ (ê²¹ì¹¨ ì—†ìŒ)
#     sampling_rate: int = 120,
#     scenes="Outside",            # None=ì „ì²´, str ë˜ëŠ” list[str]
#     original_hz: int = 120,      # ì›ë³¸ ì €ì¥ ì£¼íŒŒìˆ˜(ê¸°ë³¸ 120Hzë¡œ ê°€ì •)
#     save_meta: bool = True,
#     # ---- (ì‹ ê·œ) íƒ€ê¹ƒ ìŠ¤ë¬´ë”© ì˜µì…˜ (6) ----
#     enable_target_smoothing: bool = False,
#     target_smoothing_method: str = "ema",  # "ema" | "median"
#     target_smoothing_steps: int = 3,       # 3~5 ê¶Œì¥ (ìƒ˜í”Œ ë‹¨ìœ„; 120Hzë©´ 3=25ms*3ê°€ ì•„ë‹˜ì— ìœ ì˜, ë‹¤ìš´ìƒ˜í”Œ í›„ ê¸°ì¤€)
#     smooth_before_zscore: bool = True,
#     # ---- (ì‹ ê·œ) í”¼ì²˜ í™•ì¥ ì˜µì…˜ (4) ----
#     enable_feature_expansion: bool = False,
#     fe_diff_orders=(1, 2),                 # 1ì°¨, 2ì°¨ ì°¨ë¶„ ì±„ë„ ì¶”ê°€
#     fe_ma_seconds=(2,),                    # ì´ë™í‰ê·  ì´ˆ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: (2,5))
#     fe_std_seconds=(5,),                   # ì´ë™í‘œì¤€í¸ì°¨ ì´ˆ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸
#     fe_enable_slope=True,                  # ì°½ ì „ì²´ ê¸°ìš¸ê¸° ì±„ë„(ìƒìˆ˜ì±„ë„)
#     fe_enable_iqr=True,                    # ì°½ ì „ì²´ IQR ì±„ë„(ìƒìˆ˜ì±„ë„)
#     fe_enable_band_energy=True,            # FFT ëŒ€ì—­ ì—ë„ˆì§€ ì±„ë„(ìƒìˆ˜ì±„ë„)
#     # ==== âœ… ì‹ ê·œ: ì €ì—­ ë²ˆë“¤ ì˜µì…˜ ====
#     fe_enable_lowfreq_bundle: bool = False,  # ê¸°ë³¸ OFF
#     fe_lowfreq_hop_seconds: float = None,    # Noneì´ë©´ stride_secondsë¡œ ê°„ì£¼
#     fe_lowfreq_targets: tuple = ("EDA_Tonic","EDA_Phasic","PPG_Rate","RSP_Rate","pupilL"),
#     fe_lowfreq_spec: dict = None,  # {"ema_taus":[10.0], "slope_secs":[10.0], "use_bandpower": False}
#     lf_ema_seconds: tuple = (10,),      # Ï„(s) ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: (6,10,20))
#     lf_slope_seconds: tuple = (10,),    # Rolling slope ì°½ ê¸¸ì´(sec)
#     # Nyquist=0.5*sampling_rate ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •. hop=2s â†’ fs=0.5Hzë©´ Nyq=0.25
#     lf_bands: tuple = ((0.00, 0.02, "LF"), (0.02, 0.08, "MF"), (0.08, 0.20, "HF")),
# ):
#     """
#     - ì…ë ¥ í´ë”ì˜ {pid}_Main.pkl ë¡œë¶€í„° sceneë³„ë¡œ ì›ì‹œ ì‹ í˜¸ë¥¼ ìœˆë„ì‰.
#     - ì¶œë ¥: X_array [N,C,T], y_array [N], pid_array [N], scene_array [N], windex_array [N]
#     - feature_tag_list.npy: ì‚¬ìš©ëœ ì±„ë„ ì´ë¦„
#     - meta.json: íŒŒë¼ë¯¸í„°/ìš”ì•½ ì •ë³´(ì˜µì…˜)

#     ë³€ê²½ì :
#       â€¢ enable_target_smoothing: Trueë©´ yì— EMA/Median í•„í„° ì ìš© (ë…¸ì´ì¦ˆ ì™„í™”)
#       â€¢ enable_feature_expansion: Trueë©´ ê° ì±„ë„ì— ì‹œê³„ì—´ íŒŒìƒ/ìƒìˆ˜ íŠ¹ì„± ì±„ë„ ì¶”ê°€
#         - ì°¨ë¶„(1,2), ì´ë™í‰ê· /í‘œì¤€í¸ì°¨, slope, IQR, FFT ëŒ€ì—­ì—ë„ˆì§€
#     """
#     os.makedirs(output_path, exist_ok=True)

#     # ìœˆë„/ìŠ¤íŠ¸ë¼ì´ë“œ ìƒ˜í”Œ ìˆ˜
#     window_size = int(window_seconds * sampling_rate)
#     stride_size = int(stride_seconds * sampling_rate)

#     # ì‚¬ìš© ì‹ í˜¸ ì»¬ëŸ¼ (íŒŒìƒ í”¼ì²˜ ìœ„ì£¼)
#     signal_dict = {
#         "EDA":   ["EDA_Tonic", "EDA_Phasic", "SCR_Amplitude", "SCR_RiseTime"],
#         "PPG":   ["PPG_Rate"],  # HRV ì£¼íŒŒìˆ˜ëŒ€ì—­ì€ RRì´ ì—†ìœ¼ë¯€ë¡œ PPG_Rateë¡œ ê·¼ì‚¬(ì£¼ì˜)
#         "RSP":   ["RSP_Rate", "RSP_RVT", "RSP_Amplitude"],
#         "Pupil": ["pupilL", "pupilR", "pupil_mean"],
#     }
#     base_cols = sum(signal_dict.values(), [])  # í‰íƒ„í™”

#     # ìŠ¤í™íŠ¸ëŸ¼ ëŒ€ì—­ ì •ì˜ (ëª¨ë‹¬ë¦¬í‹°ë³„ ê¶Œì¥ì¹˜)
#     # - EDA tonic: 0â€“0.4Hz
#     # - RSP band: 0.2â€“0.5Hz
#     # - HRV ê·¼ì‚¬(LF/HF): 0.04â€“0.15, 0.15â€“0.4 (PPG_Rate ê¸°ë°˜ ê·¼ì‚¬)
#     band_map = {
#         "EDA_Tonic": [(0.0, 0.4, "EDA_0_0.4")],
#         "RSP_Rate":  [(0.2, 0.5, "RSP_0.2_0.5")],
#         "RSP_RVT":   [(0.2, 0.5, "RSP_0.2_0.5")],
#         "RSP_Amplitude": [(0.2, 0.5, "RSP_0.2_0.5")],
#         "PPG_Rate":  [(0.04, 0.15, "HRV_LF_approx"),
#                       (0.15, 0.40, "HRV_HF_approx")],
#         # pupilì€ ìŠ¤í™íŠ¸ëŸ¼ ê¸°ë³¸ OFF (ì›í•˜ë©´ ì¶”ê°€)
#     }

#     # ì°¸ê°€ì ëª©ë¡
#     participants = sorted([f.split("_")[0] for f in os.listdir(data_path) if f.endswith("_Main.pkl")])

#     # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
#     X_list, y_list, pid_list = [], [], []
#     scene_list, windex_list = [], []

#     # scenes ì¸ì ì •ê·œí™”
#     if scenes is None:
#         scenes_set = None  # ëª¨ë“  scene í—ˆìš©
#     elif isinstance(scenes, str):
#         scenes_set = {scenes}
#     else:
#         scenes_set = set(scenes)

#     # ë¡¤ë§ ì»¤ë„ í¬ê¸° (ìƒ˜í”Œ ë‹¨ìœ„) ì¤€ë¹„
#     ma_ks = [max(1, int(round(s * sampling_rate))) for s in fe_ma_seconds]
#     std_ks = [max(1, int(round(s * sampling_rate))) for s in fe_std_seconds]

#     for pid in tqdm(participants, desc="Extracting Raw Signals"):
#         try:
#             df = pd.read_pickle(os.path.join(data_path, f"{pid}_Main.pkl"))

#             if 'scene' not in df.columns:
#                 df['scene'] = 'unknown'

#             # scene í•„í„°ë§
#             if scenes_set is None:
#                 df_scene_all = df.copy()
#             else:
#                 df_scene_all = df[df['scene'].isin(scenes_set)].copy()

#             if df_scene_all.empty or "anxiety" not in df_scene_all.columns:
#                 continue

#             # pupil_mean ìƒì„± (ì—†ìœ¼ë©´)
#             if "pupil_mean" not in df_scene_all.columns and {"pupilL", "pupilR"}.issubset(df_scene_all.columns):
#                 df_scene_all["pupil_mean"] = df_scene_all[["pupilL", "pupilR"]].mean(axis=1)

#             # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€ + ê²°ì¸¡ ì œê±° (scene í¬í•¨)
#             keep_cols = ["scene", "anxiety"] + [c for c in base_cols if c in df_scene_all.columns]
#             df_scene_all = df_scene_all[keep_cols].dropna().reset_index(drop=True)
#             if len(df_scene_all) < window_size:
#                 continue

#             # ë‹¤ìš´ìƒ˜í”Œ (í•„ìš” ì‹œ)
#             if sampling_rate < original_hz:
#                 df_scene_all = interpolate_downsample(
#                     df_scene_all, target_hz=sampling_rate, original_hz=original_hz
#                 )

#             # ---- íƒ€ê¹ƒ ìŠ¤ë¬´ë”©(ì˜µì…˜) ----
#             anxiety_raw = df_scene_all["anxiety"].to_numpy().astype(np.float32)
#             if enable_target_smoothing:
#                 k = max(1, int(target_smoothing_steps))
#                 if target_smoothing_method.lower() == "median":
#                     # ê°„ë‹¨ median filter (ê¸¸ì´ k, í™€ìˆ˜ ê°•ì œ)
#                     if k % 2 == 0: k += 1
#                     pad = k // 2
#                     xp = np.pad(anxiety_raw, (pad, pad), mode="reflect")
#                     sm = np.array([np.median(xp[i:i+k]) for i in range(len(xp)-k+1)], dtype=np.float32)
#                 else:
#                     # EMA
#                     alpha = 2.0 / (k + 1.0)
#                     sm = np.empty_like(anxiety_raw)
#                     acc = anxiety_raw[0]
#                     for i, v in enumerate(anxiety_raw):
#                         acc = alpha * v + (1 - alpha) * acc
#                         sm[i] = acc
#                 anxiety_for_norm = sm if smooth_before_zscore else anxiety_raw
#             else:
#                 anxiety_for_norm = anxiety_raw

#             # Z-score (ì”¬ í•„í„° í›„ ì „ì²´ êµ¬ê°„ ê¸°ì¤€)
#             a_mean, a_std = np.nanmean(anxiety_for_norm), np.nanstd(anxiety_for_norm)
#             a_std = a_std if a_std > 1e-6 else 1.0
#             anxiety_z = (anxiety_for_norm - a_mean) / a_std

#             # ëª¨ë“  ì‹ í˜¸ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸(ì •ì±… ìœ ì§€: ì „ë¶€ ìˆì–´ì•¼ ì§„í–‰)
#             present_cols = [c for c in base_cols if c in df_scene_all.columns]
#             if len(present_cols) != len(base_cols):
#                 continue

#             # ì°¸ê°€ìÃ—sceneë³„ ìœˆë„ ì¸ë±ìŠ¤ ì¹´ìš´í„°
#             widx_counter = defaultdict(int)

#             n = len(df_scene_all)
#             scene_series = df_scene_all['scene'].to_numpy()

#             # ì›ë³¸ ì‹œê³„ì—´ ìºì‹œ
#             series_map = {c: df_scene_all[c].to_numpy().astype(np.float32) for c in present_cols}

#             for start in range(0, n - window_size + 1, stride_size):
#                 end = start + window_size

#                 # scene ê²½ê³„ ì•ˆì „: ì°½ ë‚´ë¶€ì— ì„œë¡œ ë‹¤ë¥¸ sceneì´ ì„ì´ë©´ ìŠ¤í‚µ
#                 window_scenes = scene_series[start:end]
#                 if np.any(window_scenes != window_scenes[0]):
#                     continue
#                 sc_name = str(window_scenes[0])

#                 channel_data = []
#                 channel_tags = []

#                 # ---- ì±„ë„ë³„ í‘œì¤€í™” ì´ì „ì— íŒŒìƒ ìƒì„± (ìœˆë„ ë‚´ë¶€ì—ì„œ z-score ì ìš©) ----
#                 for col in present_cols:
#                     seg = series_map[col][start:end]  # ì›ë³¸ ì°½ (float32)

#                     # ê¸°ë³¸ ì±„ë„: seg (ë‚˜ì¤‘ì— z-score)
#                     candidates = [(seg, col)]

#                     if enable_feature_expansion:
#                         # 1) 1Â·2ì°¨ ì°¨ë¶„ (ê¸¸ì´ ë³´ì¡´ ìœ„í•´ ì•ê°’ ë³´ê°„)
#                         for od in fe_diff_orders:
#                             d = _diff(seg, order=od)
#                             candidates.append((d, f"{col}_diff{od}"))

#                         # 2) ì´ë™í‰ê·  / ì´ë™í‘œì¤€í¸ì°¨ (ê¸¸ì´ ë™ì¼)
#                         for k in ma_ks:
#                             ma = _rolling_mean(seg, k)
#                             candidates.append((ma, f"{col}_ma{k}"))
#                         for k in std_ks:
#                             rs = _rolling_std(seg, k)
#                             candidates.append((rs, f"{col}_std{k}"))

#                         # 3) slope / IQR / band energy â†’ ìŠ¤ì¹¼ë¼ â†’ ìƒìˆ˜ ì±„ë„ë¡œ í™•ì¥
#                         if fe_enable_slope:
#                             s = _slope_whole_window(seg)
#                             candidates.append((np.full_like(seg, s), f"{col}_slope"))
#                         if fe_enable_iqr:
#                             q = _iqr_whole_window(seg)
#                             candidates.append((np.full_like(seg, q), f"{col}_iqr"))
#                         if fe_enable_band_energy and col in band_map:
#                             for (flo, fhi, tag) in band_map[col]:
#                                 be = _band_energy_fft(seg, sampling_rate, flo, fhi)
#                                 candidates.append((np.full_like(seg, be), f"{col}_{tag}"))
#                         # === [PATCH 3/3] ì €ì—­ ë²ˆë“¤ (hop=2s ê¸°ì¤€, ë°´ë“œíŒŒì›Œ OFF ê¸°ë³¸) ===================
#                         if fe_enable_lowfreq_bundle:
#                             # 3-1) hop seconds ê²°ì •: ì§€ì • ì—†ìœ¼ë©´ stride_seconds ê·¸ëŒ€ë¡œ ì‚¬ìš©
#                             _hop_sec = float(fe_lowfreq_hop_seconds) if fe_lowfreq_hop_seconds is not None else float(stride_seconds)

#                             # 3-2) ëŒ€ìƒ ì»¬ëŸ¼ë§Œ ì ìš© (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì¼ ë•Œë§Œ)
#                             apply_lowfreq = (col in fe_lowfreq_targets)

#                             # 3-3) spec ê¸°ë³¸ê°’
#                             _spec = dict(ema_taus=[10.0], slope_secs=[10.0], use_bandpower=False)
#                             if isinstance(fe_lowfreq_spec, dict):
#                                 _spec.update(fe_lowfreq_spec)

#                             if apply_lowfreq:
#                                 # (A) EMA(Ï„ in seconds) : causal, hopê¸°ë°˜
#                                 for tau in _spec.get("ema_taus", []) or []:
#                                     ema = _ema_causal_hop(seg, hop_seconds=_hop_sec, tau_seconds=float(tau))
#                                     candidates.append((ema.astype(np.float32), f"{col}__LF_EMA_{int(round(tau))}s"))

#                                 # (B) Rolling slope(window seconds) : causal, per-second slope
#                                 for wsec in _spec.get("slope_secs", []) or []:
#                                     rs = _rolling_slope_causal_hop(seg, hop_seconds=_hop_sec, window_seconds=float(wsec))
#                                     candidates.append((rs.astype(np.float32), f"{col}__LF_RSLOPE_{int(round(wsec))}s"))

#                                 # (C) ë°´ë“œíŒŒì›Œ/ë¹„ìœ¨ì€ ì•ˆì „íŒì—ì„  OFF (ì›í•˜ë©´ Trueë¡œ)
#                                 if bool(_spec.get("use_bandpower", False)):
#                                     # ë°´ë“œíŒŒì›Œë¥¼ ì“°ëŠ” ê²½ìš°ë¼ë„ 'fs_eff = 1/hop' ì´ì–´ì•¼ í•¨.
#                                     # êµ¬í˜„ì€ ì˜ë„ì ìœ¼ë¡œ ìƒëµ: ì´ˆê¸° ê²€ì¦ì€ EMA/RSlope 2ê°œë§Œ ê¶Œì¥.
#                                     pass
#                         # ================================================================================

#                     # í›„ë³´ë“¤ì„ ê°ì ìœˆë„ ë‚´ z-score í›„ ì¶”ê°€
#                     for arr, tag in candidates:
#                         m = float(arr.mean())
#                         s = float(arr.std())
#                         s = s if s > 1e-6 else 1.0
#                         channel_data.append(((arr - m) / s).astype(np.float32))
#                         channel_tags.append(tag)

#                 X = np.stack(channel_data, axis=0)     # [C, T]
#                 y = anxiety_z[start:end].mean()        # window í‰ê·  anxiety (z)

#                 # ë©”íƒ€ ê¸°ë¡
#                 widx = widx_counter[(pid, sc_name)]
#                 widx_counter[(pid, sc_name)] += 1

#                 X_list.append(X)
#                 y_list.append(y)
#                 pid_list.append(pid)
#                 scene_list.append(sc_name)
#                 windex_list.append(widx)

#         except Exception as e:
#             print(f"[{pid}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
#             continue

#     if len(X_list) == 0:
#         print("âš ï¸ ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. scene í•„í„°/ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
#         return

#     X_array = np.asarray(X_list, dtype=np.float32)         # [N, C, T]
#     y_array = np.asarray(y_list, dtype=np.float32)         # [N]
#     pid_array = np.asarray(pid_list)                       # [N]
#     scene_array = np.asarray(scene_list)                   # [N]
#     windex_array = np.asarray(windex_list, dtype=np.int32) # [N]

#     # feature_tag_list: ë§ˆì§€ë§‰ ìœˆë„ì—ì„œì˜ channel_tags ì‚¬ìš© (ëª¨ë“  ì°½ ë™ì¼ êµ¬ì„± ê°€ì •)
#     feature_tags = np.array(channel_tags, dtype="U128")

#     np.save(os.path.join(output_path, "X_array.npy"), X_array)
#     np.save(os.path.join(output_path, "y_array.npy"), y_array)
#     np.save(os.path.join(output_path, "pid_array.npy"), pid_array)
#     np.save(os.path.join(output_path, "scene_array.npy"), scene_array)
#     np.save(os.path.join(output_path, "windex_array.npy"), windex_array)
#     np.save(os.path.join(output_path, "feature_tag_list.npy"), feature_tags)

#     if save_meta:
#         meta = {
#             "sampling_rate": sampling_rate,
#             "original_hz": original_hz,
#             "window_seconds": window_seconds,
#             "stride_seconds": stride_seconds,
#             "scenes": list(scenes_set) if scenes_set is not None else "ALL",
#             "n_windows": int(len(X_array)),
#             "n_participants": int(len(np.unique(pid_array))),
#             "enable_target_smoothing": enable_target_smoothing,
#             "target_smoothing_method": target_smoothing_method,
#             "target_smoothing_steps": int(target_smoothing_steps),
#             "smooth_before_zscore": smooth_before_zscore,
#             "enable_feature_expansion": enable_feature_expansion,
#             "fe_diff_orders": list(fe_diff_orders),
#             "fe_ma_seconds": list(fe_ma_seconds),
#             "fe_std_seconds": list(fe_std_seconds),
#             "fe_enable_slope": fe_enable_slope,
#             "fe_enable_iqr": fe_enable_iqr,
#             "fe_enable_band_energy": fe_enable_band_energy,
#             "feature_cols_base": base_cols,
#             "feature_cols_final": feature_tags.tolist()
#         }
#         with open(os.path.join(output_path, "meta.json"), "w", encoding="utf-8") as f:
#             json.dump(meta, f, ensure_ascii=False, indent=2)

#     print("âœ… ì €ì¥ ì™„ë£Œ:", output_path)
#     print(f"ğŸ“Š X shape: {X_array.shape} | y shape: {y_array.shape} | #PIDs: {len(np.unique(pid_array))}")
#     print(f"ğŸ§© Channels: {X_array.shape[1]} | (ì˜ˆ: {feature_tags[:min(10,len(feature_tags))]})")
#     print("ğŸ“ saved: scene_array.npy, windex_array.npy, feature_tag_list.npy" + (", meta.json" if save_meta else ""))
# =========================
# [ADD] Stratified PID split with scene balance + test-size constraints
# Place this block at the end of ml_dataloader.py
# =========================
from dataclasses import dataclass
from typing import Dict, Tuple, List
import numpy as np
import random
import json
import math

@dataclass
class SplitResult:
    train_m: np.ndarray
    val_m:   np.ndarray
    test_m:  np.ndarray
    info:    Dict

def _scene_distribution(mask: np.ndarray, scene: np.ndarray) -> Dict[str, float]:
    """ìœˆë„ ë§ˆìŠ¤í¬ë¡œ ì”¬ ë¶„í¬(%) ê³„ì‚°."""
    sub = scene[mask]
    if sub.size == 0:
        return {}
    vals, cnts = np.unique(sub, return_counts=True)
    total = float(cnts.sum())
    return {str(v): float(c/total*100.0) for v, c in zip(vals, cnts)}

def _pid_windows(scene: np.ndarray, pid: np.ndarray) -> Dict:
    """PIDë³„ ì”¬ë³„ ìœˆë„ ì¹´ìš´íŠ¸."""
    out = {}
    for p in np.unique(pid):
        m = (pid == p)
        vals, cnts = np.unique(scene[m], return_counts=True)
        out[p] = dict(zip([str(v) for v in vals], cnts.tolist()))
    return out

def _assign_by_pid_with_balance(
    pid: np.ndarray,
    scene: np.ndarray,
    val_ratio: float,
    *,
    min_test_pids: int,
    min_test_windows: int,
    scene_tolerance_pp: float,
    max_tries: int,
    seed: int
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:
    """
    PID ë‹¨ìœ„ë¡œ train/val/testë¥¼ í• ë‹¹.
    - ì”¬ ë¶„í¬ê°€ ê° splitì—ì„œ ì „ì²´ ë¶„í¬ ëŒ€ë¹„ Â±scene_tolerance_pp ì´ë‚´ ìœ ì§€
    - testëŠ” PIDìˆ˜/ìœˆë„ìˆ˜ í•˜í•œì„ ë§Œì¡±
    """
    rng = random.Random(seed)
    np_rng = np.random.default_rng(seed)

    unique_p = np.unique(pid)
    # í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ì€ 'val_ratioì™€ ê°™ì€ í¬ê¸°'ë¡œ ì‹œì‘(í•„ìš” ì‹œ ìë™ ì™„í™”)
    test_ratio_init = val_ratio

    # ì „ì²´ ì”¬ ë¶„í¬(ìœˆë„ ê¸°ì¤€)
    global_dist = _scene_distribution(np.ones_like(pid, dtype=bool), scene)

    pid2scene = _pid_windows(scene, pid)

    def build_masks(train_pids, val_pids, test_pids):
        train_m = np.isin(pid, train_pids)
        val_m   = np.isin(pid, val_pids)
        test_m  = np.isin(pid, test_pids)
        return train_m, val_m, test_m

    def scene_ok(train_m, val_m, test_m, tol_pp):
        for m in [train_m, val_m, test_m]:
            dist = _scene_distribution(m, scene)
            # ì”¬ì´ í•˜ë‚˜ë„ ì—†ì„ ìˆ˜ ìˆëŠ” splitì€ ì‹¤íŒ¨ ì²˜ë¦¬
            if len(dist) == 0:
                return False
            # ì „ì²´ ë¶„í¬ì™€ í¸ì°¨ ë¹„êµ
            for sc, g_pct in global_dist.items():
                s_pct = dist.get(sc, 0.0)
                if abs(s_pct - g_pct) > tol_pp:
                    return False
        return True

    # ì‹œë„ ë£¨í”„
    tries = 0
    best = None
    # ì™„í™” ê·œì¹™ ë‹¨ê³„
    # 0: tol=scene_tolerance_pp,   min_test_pids as is
    # 1: tol=scene_tolerance_pp*2, min_test_pids-2
    # 2: tol=scene_tolerance_pp*2, min_test_pids-2, test_ratio += 0.05
    relax_stage = 0

    while tries < max_tries:
        tries += 1
        # ë¬´ì‘ìœ„ PID ì…”í”Œ
        pids = unique_p.copy()
        np_rng.shuffle(pids)

        # ë¹„ìœ¨ ì„¤ì •
        test_ratio = test_ratio_init + (0.05 if relax_stage >= 2 else 0.0)

        n_total = len(pids)
        n_val   = max(1, int(round(n_total * val_ratio)))
        n_test  = max(min_test_pids, int(round(n_total * test_ratio)))
        n_val   = min(n_val, n_total - n_test - 1)  # trainì´ 1ë³´ë‹¤ ì‘ì•„ì§€ì§€ ì•Šê²Œ
        n_train = n_total - n_val - n_test
        if n_train < 1:
            continue

        # ë‹¤-ì”¬ PID ìš°ì„  ë°°ì¹˜(ì”¬ ë¶„í¬ ë§ì¶”ê¸° ì‰¬ì›€)
        pid_scene_count = [(p, len(pid2scene.get(p, {}))) for p in pids]
        pid_scene_count.sort(key=lambda x: x[1], reverse=True)
        ordered_pids = np.array([p for p, _ in pid_scene_count])

        # ì´ˆê¸° ë°°ì •: ë‹¨ìˆœ ë¹„ìœ¨ ì»·
        test_pids = set(ordered_pids[:n_test].tolist())
        val_pids  = set(ordered_pids[n_test:n_test+n_val].tolist())
        train_pids= set(ordered_pids[n_test+n_val:].tolist())

        train_m, val_m, test_m = build_masks(train_pids, val_pids, test_pids)

        # ìµœì†Œ ìœˆë„ ì¡°ê±´ í™•ì¸
        if test_m.sum() < min_test_windows:
            # ë³´ë¥˜: ë” ë§ì€ PIDë¥¼ testë¡œ ë°€ì–´ ë„£ì–´ë³¸ë‹¤
            need = min_test_windows - int(test_m.sum())
            # trainì—ì„œ ì¼ë¶€ ì´ë™
            if need > 0:
                move = min(need, len(train_pids))
                if move > 0:
                    mv = np_rng.choice(list(train_pids), size=move, replace=False)
                    for p in mv:
                        train_pids.remove(p)
                        test_pids.add(p)
                train_m, val_m, test_m = build_masks(train_pids, val_pids, test_pids)
                if test_m.sum() < min_test_windows:
                    # ì—¬ì „íˆ ë¶€ì¡± â†’ ë‹¤ìŒ ì‹œë„
                    relax_stage = min(relax_stage+1, 2)
                    continue

        # ì”¬ ë¶„í¬ í™•ì¸
        tol = scene_tolerance_pp if relax_stage == 0 else scene_tolerance_pp*2
        if not scene_ok(train_m, val_m, test_m, tol_pp=tol):
            relax_stage = min(relax_stage+1, 2)
            continue

        # í…ŒìŠ¤íŠ¸ PID í•˜í•œ í™•ì¸(ì™„í™” ë‹¨ê³„ 1ë¶€í„° -2 í—ˆìš©)
        min_test_pids_eff = min_test_pids if relax_stage == 0 else max(1, min_test_pids-2)
        if len(test_pids) < min_test_pids_eff:
            relax_stage = min(relax_stage+1, 2)
            continue

        # ì„±ê³µ
        info = {
            "tries": tries,
            "relax_stage": relax_stage,
            "global_scene_pct": global_dist,
            "train_scene_pct": _scene_distribution(train_m, scene),
            "val_scene_pct": _scene_distribution(val_m, scene),
            "test_scene_pct": _scene_distribution(test_m, scene),
            "n_pid": {
                "train": int(len(train_pids)),
                "val":   int(len(val_pids)),
                "test":  int(len(test_pids)),
                "total": int(n_total),
            },
            "n_windows": {
                "train": int(train_m.sum()),
                "val":   int(val_m.sum()),
                "test":  int(test_m.sum()),
                "total": int(len(pid)),
            },
            "pid_lists": {
                "train": sorted(list(map(int, train_pids))) if np.issubdtype(pid.dtype, np.integer) else sorted(list(train_pids)),
                "val":   sorted(list(map(int, val_pids)))   if np.issubdtype(pid.dtype, np.integer) else sorted(list(val_pids)),
                "test":  sorted(list(map(int, test_pids)))  if np.issubdtype(pid.dtype, np.integer) else sorted(list(test_pids)),
            }
        }
        best = (train_m, val_m, test_m, info)
        break

    if best is None:
        # ìµœì¢… ì‹¤íŒ¨ ì‹œ, ê°€ì¥ ë‹¨ìˆœ ë¹„ìœ¨ë¡œë¼ë„ ë°˜í™˜(ì•ˆì •ì„±)
        # (ì£¼ì˜: ì´ ê²½ìš°ì—” scene balanceë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ. ìƒìœ„ ë ˆë²¨ì—ì„œ ë¡œê·¸ë¡œ ì•Œë¦¼)
        n_total = len(unique_p)
        n_val   = max(1, int(round(n_total * val_ratio)))
        n_test  = max(min_test_pids, int(round(n_total * test_ratio_init)))
        n_val   = min(n_val, n_total - n_test - 1)
        np_rng.shuffle(unique_p)
        test_pids = set(unique_p[:n_test].tolist())
        val_pids  = set(unique_p[n_test:n_test+n_val].tolist())
        train_pids= set(unique_p[n_test+n_val:].tolist())
        train_m   = np.isin(pid, list(train_pids))
        val_m     = np.isin(pid, list(val_pids))
        test_m    = np.isin(pid, list(test_pids))
        info = {
            "tries": tries,
            "relax_stage": "FAILED_FALLBACK",
            "global_scene_pct": _scene_distribution(np.ones_like(pid, dtype=bool), scene),
            "train_scene_pct": _scene_distribution(train_m, scene),
            "val_scene_pct": _scene_distribution(val_m, scene),
            "test_scene_pct": _scene_distribution(test_m, scene),
            "n_pid": {
                "train": int(len(train_pids)),
                "val":   int(len(val_pids)),
                "test":  int(len(test_pids)),
                "total": int(len(np.unique(pid))),
            },
            "n_windows": {
                "train": int(train_m.sum()),
                "val":   int(val_m.sum()),
                "test":  int(test_m.sum()),
                "total": int(len(pid)),
            },
            "pid_lists": {
                "train": sorted(list(map(int, train_pids))) if np.issubdtype(pid.dtype, np.integer) else sorted(list(train_pids)),
                "val":   sorted(list(map(int, val_pids)))   if np.issubdtype(pid.dtype, np.integer) else sorted(list(val_pids)),
                "test":  sorted(list(map(int, test_pids)))  if np.issubdtype(pid.dtype, np.integer) else sorted(list(test_pids)),
            }
        }
        best = (train_m, val_m, test_m, info)

    return best

def split_across_with_gap_stratified(
    pid: np.ndarray,
    scene: np.ndarray,
    widx: np.ndarray,
    *,
    val_ratio: float,
    gap_steps: int,
    min_test_pids: int = 10,
    min_test_windows: int = 1000,
    scene_tolerance_pp: float = 5.0,
    max_tries: int = 200,
    seed: int = 42
):
    """
    ìƒˆ ìŠ¤í”Œë¦¿:
      1) PID ë‹¨ìœ„ ë¶„í• (LOPO)
      2) ì”¬ ë¶„í¬ ìœ ì§€(Â±scene_tolerance_pp)
      3) í…ŒìŠ¤íŠ¸ ìµœì†Œ ê·œëª¨ ë³´ì¥(min_test_pids, min_test_windows)

    ì£¼ì˜: across-participantë¼ë©´ PID ë¶ˆêµì°¨ì´ë¯€ë¡œ gapì€ ì‚¬ì‹¤ìƒ ë¬´ì˜ë¯¸í•©ë‹ˆë‹¤.
         (í˜•ì‹ ì¼ê´€ì„±ì„ ìœ„í•´ gap_stepsëŠ” ë©”íƒ€ì—ë§Œ ê¸°ë¡í•©ë‹ˆë‹¤.)
    """
    train_m, val_m, test_m, info = _assign_by_pid_with_balance(
        pid=pid, scene=scene, val_ratio=val_ratio,
        min_test_pids=min_test_pids, min_test_windows=min_test_windows,
        scene_tolerance_pp=scene_tolerance_pp, max_tries=max_tries, seed=seed
    )
    info["gap_steps"] = int(gap_steps)
    return SplitResult(train_m=train_m, val_m=val_m, test_m=test_m, info=info)
# === ml_dataloader.py or notebook ===
from sklearn.model_selection import GroupKFold
import numpy as np

def outer_pid_kfold_splits(pid, scene, widx, n_splits=5, seed=42):
    """
    PID ë‹¨ìœ„ë¡œ GroupKFold ìˆ˜í–‰ â†’ ê° foldë§ˆë‹¤ train/val/test mask ë°˜í™˜
    - valì€ train ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ 20% ëœë¤ ë¶„í• 
    """
    uniq_pids = np.unique(pid)
    gkf = GroupKFold(n_splits=n_splits)
    rng = np.random.default_rng(seed)
    folds = []

    for i, (train_pid_idx, test_pid_idx) in enumerate(gkf.split(uniq_pids, groups=uniq_pids)):
        test_pids = uniq_pids[test_pid_idx]
        train_pids = uniq_pids[train_pid_idx]

        train_mask = np.isin(pid, train_pids)
        test_mask  = np.isin(pid, test_pids)

        # train ë‚´ë¶€ì—ì„œ val_ratio=0.2 ëœë¤ ë¶„í• 
        tr_idx = np.where(train_mask)[0]
        rng.shuffle(tr_idx)
        n_val = int(len(tr_idx) * 0.2)
        val_idx = tr_idx[:n_val]

        val_mask = np.zeros_like(pid, dtype=bool)
        val_mask[val_idx] = True
        train_mask[val_idx] = False   # val ì œì™¸

        folds.append(dict(
            train_m=train_mask,
            val_m=val_mask,
            test_m=test_mask,
            info={"fold": i+1, "n_train": train_mask.sum(),
                  "n_val": val_mask.sum(), "n_test": test_mask.sum()}
        ))
    return folds

def print_split_report(
    pid: np.ndarray,
    scene: np.ndarray,
    y: np.ndarray,
    train_m: np.ndarray,
    val_m: np.ndarray,
    test_m: np.ndarray,
    title: str = "POST-LAG SPLIT (STRATIFIED)"
):
    """í•„ìˆ˜ ìš”ì•½ ë¦¬í¬íŠ¸: PID/ìœˆë„ ìˆ˜, ì”¬ ë¶„í¬, y í†µê³„(IQR)"""
    def stats(mask):
        sub = y[mask]
        if sub.size == 0:
            return {"n": 0, "mean": float("nan"), "std": float("nan"), "iqr": float("nan")}
        q75, q25 = np.percentile(sub, [75, 25])
        return {
            "n": int(sub.size),
            "mean": float(np.mean(sub)),
            "std": float(np.std(sub)),
            "iqr": float(q75 - q25),
            "n_pid": int(len(np.unique(pid[mask])))
        }
    tr = stats(train_m); va = stats(val_m); te = stats(test_m)
    tr_pct = _scene_distribution(train_m, scene)
    va_pct = _scene_distribution(val_m, scene)
    te_pct = _scene_distribution(test_m, scene)
    print(f"\n===== {title} =====")
    print(f"[PID counts] train={tr['n_pid']} | val={va['n_pid']} | test={te['n_pid']}")
    print(f"[Window counts] train={tr['n']} | val={va['n']} | test={te['n']}")
    print("[Scene %] train:", json.dumps(tr_pct, ensure_ascii=False))
    print("[Scene %]   val:", json.dumps(va_pct, ensure_ascii=False))
    print("[Scene %]  test:", json.dumps(te_pct, ensure_ascii=False))
    print(f"[y] train: mean={tr['mean']:.3f}, std={tr['std']:.3f}, IQR={tr['iqr']:.3f}")
    print(f"[y]   val: mean={va['mean']:.3f}, std={va['std']:.3f}, IQR={va['iqr']:.3f}")
    print(f"[y]  test: mean={te['mean']:.3f}, std={te['std']:.3f}, IQR={te['iqr']:.3f}\n")
