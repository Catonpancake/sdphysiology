import os
import numpy as np
import pandas as pd
import neurokit2 as nk
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")


def process_physiology_data(
    data_path,
    output_path="./ml_processed",
    window_seconds=20,
    stride_seconds=2,
    sampling_rate=120,
):
    os.makedirs(output_path, exist_ok=True)

    window_size = sampling_rate * window_seconds
    stride_size = sampling_rate * stride_seconds

    valid_cols = {
        "EDA": ["EDA_Tonic", "EDA_Phasic", "SCR_Amplitude", "SCR_RiseTime"],
        "PPG": ["PPG_Rate"],
        "RSP": ["RSP_Rate", "RSP_RVT", "RSP_Amplitude"],
        "Pupil": ["pupilL", "pupilR", "pupil_mean"]
    }

    clip_dict = {
        "EDA_Tonic": 30, "EDA_Phasic": 10, "SCR_Amplitude": 10, "SCR_RiseTime": 10,
        "PPG_Rate": 5, "RSP_Rate": 5, "RSP_RVT": 7, "RSP_Amplitude": 10,
        "pupilL": 10, "pupilR": 10, "pupil_mean": 10
    }

    participants = sorted([f.split("_")[0] for f in os.listdir(data_path) if f.endswith("_Main.pkl")])

    baseline_dict = {}
    anxiety_baseline_dict = {}
    all_features = []
    X_array = []
    y_array = []
    pid_array = []
    feature_tag_list = []

    for pid in tqdm(participants, desc="Processing"):
        try:
            df = pd.read_pickle(os.path.join(data_path, f"{pid}_Main.pkl"))
            df = df[df["scene"] == "Outside"].dropna().reset_index(drop=True)

            if "pupilL" in df.columns and "pupilR" in df.columns:
                df["pupil_mean"] = df[["pupilL", "pupilR"]].mean(axis=1)

            base = df.iloc[:sampling_rate * 10]
            baseline_dict[pid] = {
                col: (base[col].mean(), base[col].std() if base[col].std() > 1e-6 else 1.0)
                for mod, cols in valid_cols.items() for col in cols if col in base.columns
            }

            if "anxiety" in base.columns:
                mean = base["anxiety"].mean()
                std = base["anxiety"].std()
                anxiety_baseline_dict[pid] = (mean, std if std > 0.5 else 1.0)

            for start in range(0, len(df) - window_size + 1, stride_size):
                window = df.iloc[start:start + window_size].copy()
                if len(window) < window_size:
                    continue

                norm_window = window.copy()
                for mod, cols in valid_cols.items():
                    for col in cols:
                        if col in norm_window.columns and col in baseline_dict[pid]:
                            mean, std = baseline_dict[pid][col]
                            norm_window[col] = (norm_window[col] - mean) / std

                if "PPG_Clean" in norm_window.columns:
                    quality = nk.ppg_quality(norm_window["PPG_Clean"].values, sampling_rate=sampling_rate)
                    if np.nanmean(quality) < 0.5:
                        continue

                try:
                    peaks = np.where(window["PPG_Peaks"].values == 1)[0]
                    if len(peaks) >= 4:
                        ibi = np.diff(peaks) / sampling_rate
                        cv = np.std(ibi) / np.mean(ibi)
                        if cv > 0.5:
                            raise ValueError(f"High HRV CV: {cv:.2f}")
                        hrv = nk.hrv_time(peaks, sampling_rate=sampling_rate, show=False, method="time")
                        hrv_features = hrv[["HRV_RMSSD", "HRV_SDNN", "HRV_pNN50"]].iloc[0].to_dict()
                    else:
                        hrv_features = {"HRV_RMSSD": np.nan, "HRV_SDNN": np.nan, "HRV_pNN50": np.nan}
                except Exception:
                    hrv_features = {"HRV_RMSSD": np.nan, "HRV_SDNN": np.nan, "HRV_pNN50": np.nan}

                row = {"participant": pid, "start_idx": start}
                if "anxiety" in window.columns and pid in anxiety_baseline_dict:
                    mean, std = anxiety_baseline_dict[pid]
                    z_scored = (window["anxiety"] - mean) / std
                    row["anxiety"] = z_scored.mean()
                    y_array.append(z_scored.mean())

                feature_sequence = []
                feature_tags = []

                for mod, cols in valid_cols.items():
                    for col in cols:
                        if col in norm_window.columns:
                            clipped = norm_window[col].clip(-clip_dict[col], clip_dict[col])
                            row[f"{col}_mean"] = clipped.mean()
                            row[f"{col}_std"] = clipped.std()
                            row[f"{col}_max"] = clipped.max()
                            row[f"{col}_slope"] = np.polyfit(np.arange(len(clipped)), clipped, 1)[0]
                            feature_sequence.append(clipped.values)
                            feature_tags.append(f"{col}")

                if feature_sequence:
                    X_array.append(np.stack(feature_sequence, axis=1))  # [T, C]
                    pid_array.append(pid)
                    feature_tag_list.append(feature_tags)

                row.update(hrv_features)
                all_features.append(row)

        except Exception as e:
            print(f"[{pid}] ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue

    df_feat = pd.DataFrame(all_features)
    X_array = np.array(X_array)
    y_array = np.array(y_array)
    pid_array = np.array(pid_array)
    feature_tag_list = feature_tag_list[0] if feature_tag_list else []

    np.save(os.path.join(output_path, "X_array.npy"), X_array)
    np.save(os.path.join(output_path, "y_array.npy"), y_array)
    np.save(os.path.join(output_path, "pid_array.npy"), pid_array)
    np.save(os.path.join(output_path, "feature_tag_list.npy"), feature_tag_list)
    df_feat.to_csv(os.path.join(output_path, "df_feat.csv"), index=False)

    print("âœ… ì €ì¥ ì™„ë£Œ:", output_path)
    print(f"ğŸ“Š X shape: {X_array.shape} | y shape: {y_array.shape} | feature dim: {len(feature_tag_list)}")

import os
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict

# ---------------------------
# Helper: interpolation-based downsampling (column-wise)
# ---------------------------
def interpolate_downsample(df: pd.DataFrame, target_hz: int, original_hz: int = 120, time_col: str = None):
    """
    ì„ í˜•ë³´ê°„ ê¸°ë°˜ ë‹¤ìš´ìƒ˜í”Œë§. ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì²˜ë¦¬.
    time_colì´ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì»¬ëŸ¼(ë°€ë¦¬ì´ˆ) ê¸°ì¤€ìœ¼ë¡œ ë¦¬ìƒ˜í”Œ, ì—†ìœ¼ë©´ ê°€ìƒ ì‹œê°„ì¶• ì‚¬ìš©.
    target_hz >= original_hz ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    if target_hz >= original_hz:
        return df.copy()

    num_rows = len(df)
    if num_rows == 0:
        return df.copy()

    # ì›ë˜/íƒ€ê²Ÿ ì‹œê°„ì¶•
    if time_col is None:
        t_orig = np.arange(num_rows) / original_hz
    else:
        t_orig = (df[time_col].to_numpy() / 1000.0)

    total_time = t_orig[-1] - t_orig[0]
    n_target = int(np.floor(total_time * target_hz)) + 1
    if n_target < 2:
        n_target = max(2, int(num_rows * target_hz / original_hz))

    t_new = np.linspace(t_orig[0], t_orig[-1], n_target)

    # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ë³´ê°„
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    out = {}
    for col in numeric_cols:
        x = df[col].to_numpy()
        # NaN ì„ì‹œ ì±„ìš°ê¸°(ì•/ë’¤ í™•ì¥)
        if np.isnan(x).any():
            s = pd.Series(x).ffill().bfill().to_numpy()
        else:
            s = x
        out[col] = np.interp(t_new, t_orig, s)

    # ìˆ«ì ì•„ë‹Œ ì»¬ëŸ¼ì€ ìµœê·¼ì ‘ ì¸ë±ìŠ¤ë¡œ ì„œë¸Œìƒ˜í”Œ
    non_numeric_cols = [c for c in df.columns if c not in numeric_cols]
    if non_numeric_cols:
        idx_new = np.searchsorted(t_orig, t_new, side="left")
        idx_new = np.clip(idx_new, 0, num_rows - 1)
        for col in non_numeric_cols:
            out[col] = df[col].iloc[idx_new].to_numpy()

    return pd.DataFrame(out)
# ---------------------------
# Main: extract raw windows (scene ê³ ì • ì»¬ëŸ¼ëª… ì‚¬ìš©)
#  - ì¶”ê°€: feature expansion(ì˜µì…˜), target smoothing(ì˜µì…˜)
# ---------------------------
import os, json
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict

# ---- ê°„ë‹¨ ë¡¤ë§/ìŠ¤í™íŠ¸ëŸ¼ ìœ í‹¸ ----
def _rolling_mean(x, k):
    if k <= 1: return x.copy()
    # padding='reflect'ë¡œ ê°€ì¥ìë¦¬ ì™œê³¡ ìµœì†Œí™”
    pad = k // 2
    xpad = np.pad(x, (pad, k - 1 - pad), mode='reflect')
    ker = np.ones(k, dtype=np.float32) / k
    return np.convolve(xpad, ker, mode='valid').astype(np.float32)

def _rolling_std(x, k):
    if k <= 1: return np.zeros_like(x, dtype=np.float32)
    m = _rolling_mean(x, k)
    # (x-m)^2ì˜ í‰ê· ì˜ ë£¨íŠ¸
    pad = k // 2
    xpad = np.pad(x, (pad, k - 1 - pad), mode='reflect')
    ker = np.ones(k, dtype=np.float32) / k
    v = np.convolve((xpad - np.mean(xpad))**2, ker, mode='valid')
    # ê·¼ì‚¬: êµ­ì†Œë¶„ì‚° ëŒ€ì‹  ì „ì—­í‰ê·  ë³´ì • í”¼í•˜ê¸° ìœ„í•´ m ì´ìš©
    return np.sqrt(np.maximum(1e-12, _rolling_mean((x - m)**2, k))).astype(np.float32)

def _diff(x, order=1):
    if order == 1:
        d = np.diff(x, n=1, prepend=x[0])
    elif order == 2:
        d = np.diff(x, n=2, prepend=[x[0], x[1] if len(x) > 1 else x[0]])
    else:
        raise ValueError("order must be 1 or 2")
    return d.astype(np.float32)

def _slope_whole_window(x):
    # ì°½ ì „ì²´ì— ëŒ€í•´ ì„ í˜•íšŒê·€ ê¸°ìš¸ê¸° (ìƒìˆ˜ ì±„ë„ë¡œ ë°˜í™˜)
    n = len(x)
    t = np.arange(n, dtype=np.float32)
    t -= t.mean()
    denom = np.sum(t*t) + 1e-12
    slope = np.sum((x - x.mean()) * t) / denom
    return slope

def _iqr_whole_window(x):
    q75, q25 = np.percentile(x, [75, 25])
    return float(q75 - q25)

def _band_energy_fft(x, fs, f_lo, f_hi):
    # ì°½ ë‚´ FFT ê¸°ë°˜ ëŒ€ì—­ ì—ë„ˆì§€ (ìƒëŒ€ì  í•©)
    x = x.astype(np.float32)
    n = len(x)
    x = x - np.mean(x)
    X = np.fft.rfft(x)  # N/2+1
    freqs = np.fft.rfftfreq(n, d=1.0/fs)
    psd = (np.abs(X) ** 2) / (n * fs)  # ê°„ë‹¨ PSD ê·¼ì‚¬
    m = (freqs >= f_lo) & (freqs < f_hi)
    return float(np.sum(psd[m]))

def extract_raw_physio_windows(
    data_path: str,
    output_path: str = "./ml_processed_raw",
    window_seconds: int = 5,     # ê¶Œì¥: 5ì´ˆ
    stride_seconds: int = 5,     # ê¶Œì¥: 5ì´ˆ (ê²¹ì¹¨ ì—†ìŒ)
    sampling_rate: int = 120,
    scenes="Outside",            # None=ì „ì²´, str ë˜ëŠ” list[str]
    original_hz: int = 120,      # ì›ë³¸ ì €ì¥ ì£¼íŒŒìˆ˜(ê¸°ë³¸ 120Hzë¡œ ê°€ì •)
    save_meta: bool = True,
    # ---- (ì‹ ê·œ) íƒ€ê¹ƒ ìŠ¤ë¬´ë”© ì˜µì…˜ (6) ----
    enable_target_smoothing: bool = False,
    target_smoothing_method: str = "ema",  # "ema" | "median"
    target_smoothing_steps: int = 3,       # 3~5 ê¶Œì¥ (ìƒ˜í”Œ ë‹¨ìœ„; 120Hzë©´ 3=25ms*3ê°€ ì•„ë‹˜ì— ìœ ì˜, ë‹¤ìš´ìƒ˜í”Œ í›„ ê¸°ì¤€)
    smooth_before_zscore: bool = True,
    # ---- (ì‹ ê·œ) í”¼ì²˜ í™•ì¥ ì˜µì…˜ (4) ----
    enable_feature_expansion: bool = False,
    fe_diff_orders=(1, 2),                 # 1ì°¨, 2ì°¨ ì°¨ë¶„ ì±„ë„ ì¶”ê°€
    fe_ma_seconds=(2,),                    # ì´ë™í‰ê·  ì´ˆ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: (2,5))
    fe_std_seconds=(5,),                   # ì´ë™í‘œì¤€í¸ì°¨ ì´ˆ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸
    fe_enable_slope=True,                  # ì°½ ì „ì²´ ê¸°ìš¸ê¸° ì±„ë„(ìƒìˆ˜ì±„ë„)
    fe_enable_iqr=True,                    # ì°½ ì „ì²´ IQR ì±„ë„(ìƒìˆ˜ì±„ë„)
    fe_enable_band_energy=True,            # FFT ëŒ€ì—­ ì—ë„ˆì§€ ì±„ë„(ìƒìˆ˜ì±„ë„)
):
    """
    - ì…ë ¥ í´ë”ì˜ {pid}_Main.pkl ë¡œë¶€í„° sceneë³„ë¡œ ì›ì‹œ ì‹ í˜¸ë¥¼ ìœˆë„ì‰.
    - ì¶œë ¥: X_array [N,C,T], y_array [N], pid_array [N], scene_array [N], windex_array [N]
    - feature_tag_list.npy: ì‚¬ìš©ëœ ì±„ë„ ì´ë¦„
    - meta.json: íŒŒë¼ë¯¸í„°/ìš”ì•½ ì •ë³´(ì˜µì…˜)

    ë³€ê²½ì :
      â€¢ enable_target_smoothing: Trueë©´ yì— EMA/Median í•„í„° ì ìš© (ë…¸ì´ì¦ˆ ì™„í™”)
      â€¢ enable_feature_expansion: Trueë©´ ê° ì±„ë„ì— ì‹œê³„ì—´ íŒŒìƒ/ìƒìˆ˜ íŠ¹ì„± ì±„ë„ ì¶”ê°€
        - ì°¨ë¶„(1,2), ì´ë™í‰ê· /í‘œì¤€í¸ì°¨, slope, IQR, FFT ëŒ€ì—­ì—ë„ˆì§€
    """
    os.makedirs(output_path, exist_ok=True)

    # ìœˆë„/ìŠ¤íŠ¸ë¼ì´ë“œ ìƒ˜í”Œ ìˆ˜
    window_size = int(window_seconds * sampling_rate)
    stride_size = int(stride_seconds * sampling_rate)

    # ì‚¬ìš© ì‹ í˜¸ ì»¬ëŸ¼ (íŒŒìƒ í”¼ì²˜ ìœ„ì£¼)
    signal_dict = {
        "EDA":   ["EDA_Tonic", "EDA_Phasic", "SCR_Amplitude", "SCR_RiseTime"],
        "PPG":   ["PPG_Rate"],  # HRV ì£¼íŒŒìˆ˜ëŒ€ì—­ì€ RRì´ ì—†ìœ¼ë¯€ë¡œ PPG_Rateë¡œ ê·¼ì‚¬(ì£¼ì˜)
        "RSP":   ["RSP_Rate", "RSP_RVT", "RSP_Amplitude"],
        "Pupil": ["pupilL", "pupilR", "pupil_mean"],
    }
    base_cols = sum(signal_dict.values(), [])  # í‰íƒ„í™”

    # ìŠ¤í™íŠ¸ëŸ¼ ëŒ€ì—­ ì •ì˜ (ëª¨ë‹¬ë¦¬í‹°ë³„ ê¶Œì¥ì¹˜)
    # - EDA tonic: 0â€“0.4Hz
    # - RSP band: 0.2â€“0.5Hz
    # - HRV ê·¼ì‚¬(LF/HF): 0.04â€“0.15, 0.15â€“0.4 (PPG_Rate ê¸°ë°˜ ê·¼ì‚¬)
    band_map = {
        "EDA_Tonic": [(0.0, 0.4, "EDA_0_0.4")],
        "RSP_Rate":  [(0.2, 0.5, "RSP_0.2_0.5")],
        "RSP_RVT":   [(0.2, 0.5, "RSP_0.2_0.5")],
        "RSP_Amplitude": [(0.2, 0.5, "RSP_0.2_0.5")],
        "PPG_Rate":  [(0.04, 0.15, "HRV_LF_approx"),
                      (0.15, 0.40, "HRV_HF_approx")],
        # pupilì€ ìŠ¤í™íŠ¸ëŸ¼ ê¸°ë³¸ OFF (ì›í•˜ë©´ ì¶”ê°€)
    }

    # ì°¸ê°€ì ëª©ë¡
    participants = sorted([f.split("_")[0] for f in os.listdir(data_path) if f.endswith("_Main.pkl")])

    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    X_list, y_list, pid_list = [], [], []
    scene_list, windex_list = [], []

    # scenes ì¸ì ì •ê·œí™”
    if scenes is None:
        scenes_set = None  # ëª¨ë“  scene í—ˆìš©
    elif isinstance(scenes, str):
        scenes_set = {scenes}
    else:
        scenes_set = set(scenes)

    # ë¡¤ë§ ì»¤ë„ í¬ê¸° (ìƒ˜í”Œ ë‹¨ìœ„) ì¤€ë¹„
    ma_ks = [max(1, int(round(s * sampling_rate))) for s in fe_ma_seconds]
    std_ks = [max(1, int(round(s * sampling_rate))) for s in fe_std_seconds]

    for pid in tqdm(participants, desc="Extracting Raw Signals"):
        try:
            df = pd.read_pickle(os.path.join(data_path, f"{pid}_Main.pkl"))

            if 'scene' not in df.columns:
                df['scene'] = 'unknown'

            # scene í•„í„°ë§
            if scenes_set is None:
                df_scene_all = df.copy()
            else:
                df_scene_all = df[df['scene'].isin(scenes_set)].copy()

            if df_scene_all.empty or "anxiety" not in df_scene_all.columns:
                continue

            # pupil_mean ìƒì„± (ì—†ìœ¼ë©´)
            if "pupil_mean" not in df_scene_all.columns and {"pupilL", "pupilR"}.issubset(df_scene_all.columns):
                df_scene_all["pupil_mean"] = df_scene_all[["pupilL", "pupilR"]].mean(axis=1)

            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€ + ê²°ì¸¡ ì œê±° (scene í¬í•¨)
            keep_cols = ["scene", "anxiety"] + [c for c in base_cols if c in df_scene_all.columns]
            df_scene_all = df_scene_all[keep_cols].dropna().reset_index(drop=True)
            if len(df_scene_all) < window_size:
                continue

            # ë‹¤ìš´ìƒ˜í”Œ (í•„ìš” ì‹œ)
            if sampling_rate < original_hz:
                df_scene_all = interpolate_downsample(
                    df_scene_all, target_hz=sampling_rate, original_hz=original_hz
                )

            # ---- íƒ€ê¹ƒ ìŠ¤ë¬´ë”©(ì˜µì…˜) ----
            anxiety_raw = df_scene_all["anxiety"].to_numpy().astype(np.float32)
            if enable_target_smoothing:
                k = max(1, int(target_smoothing_steps))
                if target_smoothing_method.lower() == "median":
                    # ê°„ë‹¨ median filter (ê¸¸ì´ k, í™€ìˆ˜ ê°•ì œ)
                    if k % 2 == 0: k += 1
                    pad = k // 2
                    xp = np.pad(anxiety_raw, (pad, pad), mode="reflect")
                    sm = np.array([np.median(xp[i:i+k]) for i in range(len(xp)-k+1)], dtype=np.float32)
                else:
                    # EMA
                    alpha = 2.0 / (k + 1.0)
                    sm = np.empty_like(anxiety_raw)
                    acc = anxiety_raw[0]
                    for i, v in enumerate(anxiety_raw):
                        acc = alpha * v + (1 - alpha) * acc
                        sm[i] = acc
                anxiety_for_norm = sm if smooth_before_zscore else anxiety_raw
            else:
                anxiety_for_norm = anxiety_raw

            # Z-score (ì”¬ í•„í„° í›„ ì „ì²´ êµ¬ê°„ ê¸°ì¤€)
            a_mean, a_std = np.nanmean(anxiety_for_norm), np.nanstd(anxiety_for_norm)
            a_std = a_std if a_std > 1e-6 else 1.0
            anxiety_z = (anxiety_for_norm - a_mean) / a_std

            # ëª¨ë“  ì‹ í˜¸ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸(ì •ì±… ìœ ì§€: ì „ë¶€ ìˆì–´ì•¼ ì§„í–‰)
            present_cols = [c for c in base_cols if c in df_scene_all.columns]
            if len(present_cols) != len(base_cols):
                continue

            # ì°¸ê°€ìÃ—sceneë³„ ìœˆë„ ì¸ë±ìŠ¤ ì¹´ìš´í„°
            widx_counter = defaultdict(int)

            n = len(df_scene_all)
            scene_series = df_scene_all['scene'].to_numpy()

            # ì›ë³¸ ì‹œê³„ì—´ ìºì‹œ
            series_map = {c: df_scene_all[c].to_numpy().astype(np.float32) for c in present_cols}

            for start in range(0, n - window_size + 1, stride_size):
                end = start + window_size

                # scene ê²½ê³„ ì•ˆì „: ì°½ ë‚´ë¶€ì— ì„œë¡œ ë‹¤ë¥¸ sceneì´ ì„ì´ë©´ ìŠ¤í‚µ
                window_scenes = scene_series[start:end]
                if np.any(window_scenes != window_scenes[0]):
                    continue
                sc_name = str(window_scenes[0])

                channel_data = []
                channel_tags = []

                # ---- ì±„ë„ë³„ í‘œì¤€í™” ì´ì „ì— íŒŒìƒ ìƒì„± (ìœˆë„ ë‚´ë¶€ì—ì„œ z-score ì ìš©) ----
                for col in present_cols:
                    seg = series_map[col][start:end]  # ì›ë³¸ ì°½ (float32)

                    # ê¸°ë³¸ ì±„ë„: seg (ë‚˜ì¤‘ì— z-score)
                    candidates = [(seg, col)]

                    if enable_feature_expansion:
                        # 1) 1Â·2ì°¨ ì°¨ë¶„ (ê¸¸ì´ ë³´ì¡´ ìœ„í•´ ì•ê°’ ë³´ê°„)
                        for od in fe_diff_orders:
                            d = _diff(seg, order=od)
                            candidates.append((d, f"{col}_diff{od}"))

                        # 2) ì´ë™í‰ê·  / ì´ë™í‘œì¤€í¸ì°¨ (ê¸¸ì´ ë™ì¼)
                        for k in ma_ks:
                            ma = _rolling_mean(seg, k)
                            candidates.append((ma, f"{col}_ma{k}"))
                        for k in std_ks:
                            rs = _rolling_std(seg, k)
                            candidates.append((rs, f"{col}_std{k}"))

                        # 3) slope / IQR / band energy â†’ ìŠ¤ì¹¼ë¼ â†’ ìƒìˆ˜ ì±„ë„ë¡œ í™•ì¥
                        if fe_enable_slope:
                            s = _slope_whole_window(seg)
                            candidates.append((np.full_like(seg, s), f"{col}_slope"))
                        if fe_enable_iqr:
                            q = _iqr_whole_window(seg)
                            candidates.append((np.full_like(seg, q), f"{col}_iqr"))
                        if fe_enable_band_energy and col in band_map:
                            for (flo, fhi, tag) in band_map[col]:
                                be = _band_energy_fft(seg, sampling_rate, flo, fhi)
                                candidates.append((np.full_like(seg, be), f"{col}_{tag}"))

                    # í›„ë³´ë“¤ì„ ê°ì ìœˆë„ ë‚´ z-score í›„ ì¶”ê°€
                    for arr, tag in candidates:
                        m = float(arr.mean())
                        s = float(arr.std())
                        s = s if s > 1e-6 else 1.0
                        channel_data.append(((arr - m) / s).astype(np.float32))
                        channel_tags.append(tag)

                X = np.stack(channel_data, axis=0)     # [C, T]
                y = anxiety_z[start:end].mean()        # window í‰ê·  anxiety (z)

                # ë©”íƒ€ ê¸°ë¡
                widx = widx_counter[(pid, sc_name)]
                widx_counter[(pid, sc_name)] += 1

                X_list.append(X)
                y_list.append(y)
                pid_list.append(pid)
                scene_list.append(sc_name)
                windex_list.append(widx)

        except Exception as e:
            print(f"[{pid}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    if len(X_list) == 0:
        print("âš ï¸ ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. scene í•„í„°/ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    X_array = np.asarray(X_list, dtype=np.float32)         # [N, C, T]
    y_array = np.asarray(y_list, dtype=np.float32)         # [N]
    pid_array = np.asarray(pid_list)                       # [N]
    scene_array = np.asarray(scene_list)                   # [N]
    windex_array = np.asarray(windex_list, dtype=np.int32) # [N]

    # feature_tag_list: ë§ˆì§€ë§‰ ìœˆë„ì—ì„œì˜ channel_tags ì‚¬ìš© (ëª¨ë“  ì°½ ë™ì¼ êµ¬ì„± ê°€ì •)
    feature_tags = np.array(channel_tags, dtype="U128")

    np.save(os.path.join(output_path, "X_array.npy"), X_array)
    np.save(os.path.join(output_path, "y_array.npy"), y_array)
    np.save(os.path.join(output_path, "pid_array.npy"), pid_array)
    np.save(os.path.join(output_path, "scene_array.npy"), scene_array)
    np.save(os.path.join(output_path, "windex_array.npy"), windex_array)
    np.save(os.path.join(output_path, "feature_tag_list.npy"), feature_tags)

    if save_meta:
        meta = {
            "sampling_rate": sampling_rate,
            "original_hz": original_hz,
            "window_seconds": window_seconds,
            "stride_seconds": stride_seconds,
            "scenes": list(scenes_set) if scenes_set is not None else "ALL",
            "n_windows": int(len(X_array)),
            "n_participants": int(len(np.unique(pid_array))),
            "enable_target_smoothing": enable_target_smoothing,
            "target_smoothing_method": target_smoothing_method,
            "target_smoothing_steps": int(target_smoothing_steps),
            "smooth_before_zscore": smooth_before_zscore,
            "enable_feature_expansion": enable_feature_expansion,
            "fe_diff_orders": list(fe_diff_orders),
            "fe_ma_seconds": list(fe_ma_seconds),
            "fe_std_seconds": list(fe_std_seconds),
            "fe_enable_slope": fe_enable_slope,
            "fe_enable_iqr": fe_enable_iqr,
            "fe_enable_band_energy": fe_enable_band_energy,
            "feature_cols_base": base_cols,
            "feature_cols_final": feature_tags.tolist()
        }
        with open(os.path.join(output_path, "meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

    print("âœ… ì €ì¥ ì™„ë£Œ:", output_path)
    print(f"ğŸ“Š X shape: {X_array.shape} | y shape: {y_array.shape} | #PIDs: {len(np.unique(pid_array))}")
    print(f"ğŸ§© Channels: {X_array.shape[1]} | (ì˜ˆ: {feature_tags[:min(10,len(feature_tags))]})")
    print("ğŸ“ saved: scene_array.npy, windex_array.npy, feature_tag_list.npy" + (", meta.json" if save_meta else ""))
